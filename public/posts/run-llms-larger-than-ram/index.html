<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=theme-color><title>How to Run LLMs Larger than RAM &#183; Ryan A. Gibson</title><meta name=title content="How to Run LLMs Larger than RAM &#183; Ryan A. Gibson"><meta name=description content="A short experiment on running larger LLMs on low-end consumer hardware, with comments on performance trade-offs and practicality."><meta name=keywords content="machine-learning,large-language-models,linux,"><link rel=canonical href=https://ryanagibson.com/posts/run-llms-larger-than-ram/><meta name=author content="Ryan Gibson"><link href=https://github.com/ragibson rel=me><link href=https://www.linkedin.com/in/ryan-a-gibson rel=me><link href=https://www.youtube.com/@ragibson rel=me><link href=https://bsky.app/profile/ragibson.bsky.social rel=me><link href=https://x.com/ryanalexgibson rel=me><link href="https://scholar.google.com/citations?user=3UZv0skAAAAJ" rel=me><link href=https://orcid.org/0000-0002-8238-7135 rel=me><link href=https://www.researchgate.net/profile/Ryan-Gibson-23 rel=me><link href=https://ko-fi.com/ragibson rel=me><meta property="og:url" content="https://ryanagibson.com/posts/run-llms-larger-than-ram/"><meta property="og:site_name" content="Ryan A. Gibson"><meta property="og:title" content="How to Run LLMs Larger than RAM"><meta property="og:description" content="A short experiment on running larger LLMs on low-end consumer hardware, with comments on performance trade-offs and practicality."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-30T21:56:47-04:00"><meta property="article:modified_time" content="2025-02-03T19:45:25-05:00"><meta property="article:tag" content="Machine-Learning"><meta property="article:tag" content="Large-Language-Models"><meta property="article:tag" content="Linux"><meta property="og:image" content="https://ryanagibson.com/posts/run-llms-larger-than-ram/feature.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ryanagibson.com/posts/run-llms-larger-than-ram/feature.webp"><meta name=twitter:title content="How to Run LLMs Larger than RAM"><meta name=twitter:description content="A short experiment on running larger LLMs on low-end consumer hardware, with comments on performance trade-offs and practicality."><link type=text/css rel=stylesheet href=/css/main.bundle.min.2442b8cd1c6b01b58a69d51f3fc6aa31ce5e944d422858b90cf7344933def382c594cd34c370b8e214b8bd06a6514c181632d20e2858c6098a1d04ac43208041.css integrity="sha512-JEK4zRxrAbWKadUfP8aqMc5elE1CKFi5DPc0STPe84LFlM00w3C44hS4vQamUUwYFjLSDihYxgmKHQSsQyCAQQ=="><script type=text/javascript src=/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js integrity="sha512-b0EXSzoFtoCCD+CMrb+l+3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script><script src=/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7+kfJ6kKCJxQGC+8wm+Bz9JucDjDTGNew=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.0e2e7ce1e319db5369cd4badb8265de52c78d08df54617521bdac9c8cd335eea6133a82c83f9cb19baf67b8133335130e7e8ff7cfeaf7171016648dbf5a0578a.js integrity="sha512-Di584eMZ21NpzUutuCZd5Sx40I31RhdSG9rJyM0zXuphM6gsg/nLGbr2e4EzM1Ew5+j/fP6vcXEBZkjb9aBXig==" data-copy=Copy data-copied=Copied></script><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"How to Run LLMs Larger than RAM","headline":"How to Run LLMs Larger than RAM","inLanguage":"en","url":"https://ryanagibson.com/posts/run-llms-larger-than-ram/","author":{"@type":"Person","name":"Ryan Gibson"},"copyrightYear":"2024","dateCreated":"2024-09-30T21:56:47-04:00","datePublished":"2024-09-30T21:56:47-04:00","dateModified":"2025-02-03T19:45:25-05:00","keywords":["machine-learning","large-language-models","linux"],"mainEntityOfPage":"true","wordCount":"914"}]</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-D1L86ZY8FN"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D1L86ZY8FN")</script></head><body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral bf-scrollbar"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
Skip to main content</a></div><div class="main-menu flex items-center w-full gap-2 p-1 pl-0"><a href=/ class="text-base font-medium truncate min-w-0 shrink">Ryan A. Gibson</a><div class="flex items-center ms-auto"><div class="hidden md:flex"><nav class="flex items-center gap-x-5 h-12"><a href=/about/ class="flex items-center bf-icon-color-hover" aria-label="About me" title="About me"><span class="text-base font-medium break-normal">About me
</span></a><a href=/posts/ class="flex items-center bf-icon-color-hover" aria-label=Posts title=Posts><span class="text-base font-medium break-normal">Posts
</span></a><a href=/projects/ class="flex items-center bf-icon-color-hover" aria-label=Projects title=Projects><span class="text-base font-medium break-normal">Projects
</span></a><a href=/extra/ class="flex items-center bf-icon-color-hover" aria-label=Extra title=Extra><span class="text-base font-medium break-normal">Extra
</span></a><button id=search-button aria-label=Search class="text-base bf-icon-color-hover" title="Search (/)">
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base bf-icon-color-hover"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav></div><div class="flex md:hidden"><div class="flex items-center h-14 gap-4"><button id=search-button-mobile aria-label=Search class="flex items-center justify-center bf-icon-color-hover" title="Search (/)">
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile type=button aria-label="Dark mode switcher" class="flex items-center justify-center text-neutral-900 hover:text-primary-600 dark:text-neutral-200 dark:hover:text-primary-400"><div class=dark:hidden><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="hidden dark:block"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button>
<input type=checkbox id=mobile-menu-toggle autocomplete=off class="hidden peer">
<label for=mobile-menu-toggle class="flex items-center justify-center cursor-pointer bf-icon-color-hover"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></label><div role=dialog aria-modal=true style=scrollbar-gutter:stable class="fixed inset-0 z-50 invisible overflow-y-auto px-6 py-20 opacity-0 transition-[opacity,visibility] duration-300 peer-checked:visible peer-checked:opacity-100 bg-neutral-50/97 dark:bg-neutral-900/99
bf-scrollbar"><label for=mobile-menu-toggle class="fixed end-8 top-5 flex items-center justify-center z-50 h-12 w-12 cursor-pointer select-none rounded-full bf-icon-color-hover border bf-border-color bf-border-color-hover bg-neutral-50 dark:bg-neutral-900"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></label><nav class="mx-auto max-w-md space-y-6"><div class=px-2><a href=/about/ aria-label="About me" class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200"><span title="About me" class="text-2xl font-bold tracking-tight">About me</span></a></div><div class=px-2><a href=/posts/ aria-label=Posts class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200"><span title=Posts class="text-2xl font-bold tracking-tight">Posts</span></a></div><div class=px-2><a href=/projects/ aria-label=Projects class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200"><span title=Projects class="text-2xl font-bold tracking-tight">Projects</span></a></div><div class=px-2><a href=/extra/ aria-label=Extra class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200"><span title=Extra class="text-2xl font-bold tracking-tight">Extra</span></a></div></nav></div></div></div></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">How to Run LLMs Larger than RAM</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-30T21:56:47-04:00>30 September 2024</time><span class="px-2 text-primary-500">&#183;</span><time datetime=2025-02-03T19:45:25-05:00>Updated: 3 February 2025</time><span class="px-2 text-primary-500">&#183;</span>
<span title="Reading time">4 mins</span></div><div class="flex flex-row flex-wrap items-center"><a class="relative mt-[0.5rem] me-2" href=/tags/machine-learning/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Machine-Learning
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/large-language-models/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large-Language-Models
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/linux/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Linux</span></span></a></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full me-4" width=96 height=96 alt="Ryan Gibson" src=/img/ryan-with-vignette.jpg data-zoom-src=/img/ryan-with-vignette.jpg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Ryan Gibson</div><div class="text-sm text-neutral-700 dark:text-neutral-400">Quantitative Analyst | Computer Scientist</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=mailto:%72ya%6E.a%6C%65xa%6E%64er.gi%62s%6Fn@%67%6Dail.c%6Fm target=_blank aria-label=Envelope title=Envelope rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M48 64C21.5 64 0 85.5.0 112c0 15.1 7.1 29.3 19.2 38.4L236.8 313.6c11.4 8.5 27 8.5 38.4.0L492.8 150.4c12.1-9.1 19.2-23.3 19.2-38.4.0-26.5-21.5-48-48-48H48zM0 176V384c0 35.3 28.7 64 64 64H448c35.3.0 64-28.7 64-64V176L294.4 339.2c-22.8 17.1-54 17.1-76.8.0L0 176z"/></svg></span></span></a>
<a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/ragibson target=_blank aria-label=Github title=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://www.linkedin.com/in/ryan-a-gibson target=_blank aria-label=Linkedin title=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://www.youtube.com/@ragibson target=_blank aria-label=Youtube title=Youtube rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentColor" d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78.0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://bsky.app/profile/ragibson.bsky.social target=_blank aria-label=Bluesky title=Bluesky rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 232.562c-21.183-41.196-78.868-117.97-132.503-155.834-51.378-36.272-70.978-29.987-83.828-24.181-14.872 6.72-17.577 29.554-17.577 42.988.0 13.433 7.365 110.138 12.169 126.281 15.873 53.336 72.376 71.358 124.413 65.574 2.66-.395 5.357-.759 8.089-1.097-2.68.429-5.379.796-8.089 1.097-76.259 11.294-143.984 39.085-55.158 137.972C201.224 526.527 237.424 403.67 256 341.379c18.576 62.291 39.972 180.718 150.734 83.983 83.174-83.983 22.851-126.674-53.408-137.969-2.71-.302-5.409-.667-8.089-1.096 2.732.337 5.429.702 8.089 1.096 52.037 5.785 108.54-12.239 124.413-65.574 4.804-16.142 12.169-112.847 12.169-126.281.0-13.434-2.705-36.267-17.577-42.988-12.85-5.806-32.45-12.09-83.829 24.181C334.868 114.595 277.183 191.366 256 232.562z"/></svg></span></span></a>
<a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/ryanalexgibson target=_blank aria-label=X-Twitter title=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a>
<a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href="https://scholar.google.com/citations?user=3UZv0skAAAAJ" target=_blank aria-label=Google-Scholar title=Google-Scholar rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 411.12.0 202.667 256 0z"/><path fill="currentColor" d="M256 411.12l256-208.453L256 0z"/><circle fill="currentColor" cx="256" cy="362.667" r="149.333"/><path fill="currentColor" d="M121.037 298.667c23.968-50.453 75.392-85.334 134.963-85.334s110.995 34.881 134.963 85.334H121.037z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://orcid.org/0000-0002-8238-7135 target=_blank aria-label=Orcid title=Orcid rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M294.75 188.19h-45.92V342h47.47c67.62.0 83.12-51.34 83.12-76.91.0-41.64-26.54-76.9-84.67-76.9zM256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm-80.79 360.76h-29.84v-207.5h29.84zm-14.92-231.14a19.57 19.57.0 1119.57-19.57 19.64 19.64.0 01-19.57 19.57zM3e2 369h-81V161.26h80.6c76.73.0 110.44 54.83 110.44 103.85C410 318.39 368.38 369 3e2 369z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://www.researchgate.net/profile/Ryan-Gibson-23 target=_blank aria-label=Researchgate title=Researchgate rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M0 32v448h448V32H0zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9.0-14.7.0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1.0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7.0 55.9 14.4 55.9 45.6.0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7v7.3zm22.9-135c-23.3.0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9.0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1.0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9zm-116.5 44.7c-9.4.0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3.0 37.2 12.2 37.2 34.5.0 21.9-15 36.6-39.7 36.6z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://ko-fi.com/ragibson target=_blank aria-label=Ko-Fi title=Ko-Fi rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg width="211.67mm" height="211.67mm" viewBox="0 0 211.67 211.67"><defs><clipPath id="clipPath25"><path d="m0 6e2h1654.8V0H0z" fill="currentColor"/></clipPath></defs><g transform="translate(-3.0786 -29.59)"><g transform="matrix(.35278 0 0 -.35278 3.0785 241.26)"><g clip-path="url(#clipPath25)"><g transform="translate(600 300)"><path d="m-3e2 3e2c-165.69.0-3e2-134.31-3e2-3e2s134.31-3e2 3e2-3e2C-134.32-3e2.0-165.69.0.0s-134.32 3e2-3e2 3e2zm-188.93-166.99h329.58c23.223.0 46.032-6.8325 65.073-20.125 18.412-12.856 36.378-33.646 42.697-67.02 14.674-77.514-43.568-135.76-126.12-131.18.438-21.268-3.8105-59.422-44.878-69.696-3.282-.821-6.6604-1.186-10.044-1.207-72.13-.448-222.53-.88916-222.53-.88916s-46.066-.01555-49.176 45.962c-1.023 77.88-.48339 223.69-.48339 223.69s-.02651 2.9267-.02051 3.6577c.043 5.607 4.4359 16.8 15.901 16.8zm312.71-55.573v-107.8s14.302-1.6507 31.907.54932c20.356 6.6 39.06 20.901 39.06 57.199.0 14.85-5.2614 25.601-11.837 33.239-9.341 10.848-23.152 16.809-37.469 16.809h-21.661z" fill="currentColor"/></g><g transform="translate(256.61 203.37)"><path d="m0 0c3.585-1.806 5.876.437 5.876.437s52.457 47.878 76.089 75.452c21.018 24.667 22.389 66.234-13.708 81.766-36.096 15.532-65.795-18.272-65.795-18.272-25.755 28.326-64.734 26.891-82.763 7.721-18.027-19.169-11.732-52.072 1.717-70.383 12.626-17.19 68.119-66.65 76.529-75.015.0.0.614-.641 2.055-1.706" fill="currentColor"/></g></g></g></g></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ms-auto px-0 lg:order-last lg:ps-8 lg:max-w-2xs"><div class="toc ps-5 print:hidden lg:sticky lg:top-10"><details open id=TOCView class="toc-right mt-0 overflow-y-auto overscroll-contain bf-scrollbar rounded-lg -ms-5 ps-5 pe-2 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted border-s-1 -ms-5 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#adding-ram-from-your-disk-swap-space>&ldquo;Adding&rdquo; RAM from your disk: swap space</a></li><li><a href=#see-also-and-references>See also and references</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg -ms-5 ps-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#adding-ram-from-your-disk-swap-space>&ldquo;Adding&rdquo; RAM from your disk: swap space</a></li><li><a href=#see-also-and-references>See also and references</a></li></ul></nav></div></details><script>(function(){"use strict";const s=.33,o="#TableOfContents",i=".anchor",a='a[href^="#"]',r="li ul",c="active";let t=!1;function l(e,n){const o=window.scrollY+window.innerHeight*n,i=[...document.querySelectorAll('#TableOfContents a[href^="#"]')],s=new Set(i.map(e=>e.getAttribute("href").substring(1)));if(t)for(let t=0;t<e.length;t++){const n=e[t];if(!s.has(n.id))continue;const o=n.getBoundingClientRect().top+window.scrollY;if(Math.abs(window.scrollY-o)<100)return n.id}for(let t=e.length-1;t>=0;t--){const n=e[t].getBoundingClientRect().top+window.scrollY;if(n<=o&&s.has(e[t].id))return e[t].id}return e.find(e=>s.has(e.id))?.id||""}function e({toc:e,anchors:t,links:n,scrollOffset:s,collapseInactive:o}){const i=l(t,s);if(!i)return;if(n.forEach(e=>{const t=e.getAttribute("href")===`#${i}`;if(e.classList.toggle(c,t),o){const n=e.closest("li")?.querySelector("ul");n&&(n.style.display=t?"":"none")}}),o){const n=e.querySelector(`a[href="#${CSS.escape(i)}"]`);let t=n;for(;t&&t!==e;)t.tagName==="UL"&&(t.style.display=""),t.tagName==="LI"&&t.querySelector("ul")?.style.setProperty("display",""),t=t.parentElement}}function n(){const n=document.querySelector(o);if(!n)return;const l=!1,u=[...document.querySelectorAll(i)],d=[...n.querySelectorAll(a)];l&&n.querySelectorAll(r).forEach(e=>e.style.display="none"),d.forEach(e=>{e.addEventListener("click",()=>{t=!0})});const c={toc:n,anchors:u,links:d,scrollOffset:s,collapseInactive:l};window.addEventListener("scroll",()=>e(c),{passive:!0}),window.addEventListener("hashchange",()=>e(c),{passive:!0}),e(c)}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",n):n()})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p>It&rsquo;s easier than ever to run large language models (LLMs) like those behind ChatGPT on your local machine, without
relying on third-party services. It&rsquo;s free and keeps everything private and confidential!</p><p>Thanks to regular open-source releases from tech giants like Meta, Google, Microsoft, and Alibaba, powerful models are
now widely available for public deployment and use.</p><p>In fact, there are impressively competent models that are lightweight enough to run on practically any smartphone.
Notably, the recent &ldquo;on-device&rdquo; <a href=https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/ target=_blank rel=noreferrer>Llama 3.2</a>
releases from Meta are only 1-2 GB in size. However, the larger models require increasingly expensive hardware to run
properly.</p><p>For example, if I try to run a moderately-sized<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> LLM on my budget $300 dev laptop with 8 GB of RAM<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, I might get
this.</p><div class=highlight-wrapper><pre tabindex=0><code class=language-commandline data-lang=commandline>$ ollama run qwen2.5:14b
Error: model requires more system memory (10.9 GiB) than is available (5.9 GiB)</code></pre></div><p>As the message says, all we need to do is add more system memory. But this doesn&rsquo;t necessarily mean that we have to go
out and upgrade our hardware.</p><h2 class="relative group">&ldquo;Adding&rdquo; RAM from your disk: swap space<div id=adding-ram-from-your-disk-swap-space class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#adding-ram-from-your-disk-swap-space aria-label=Anchor>#</a></span></h2><p>If your disk is reasonably fast, you can generally offload some memory onto it, a process known as &ldquo;swap&rdquo; on Linux (the
Windows keywords would be &ldquo;increasing the size of the page file&rdquo;). Swap space lets the kernel temporarily move inactive
pages of memory to disk, freeing up RAM for other uses.</p><p>For example, these commands will temporarily add 8 GB of usable memory from the disk.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><div class=highlight-wrapper><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo fallocate -l 8G /extra_swapfile  <span class=c1># create an 8 GB file</span>
</span></span><span class=line><span class=cl>sudo chmod <span class=m>600</span> /extra_swapfile  <span class=c1># make sure the file is only accessible by the root user</span>
</span></span><span class=line><span class=cl>sudo mkswap /extra_swapfile  <span class=c1># initialize the swap file</span>
</span></span><span class=line><span class=cl>sudo swapon /extra_swapfile  <span class=c1># activate and start using the swap file</span></span></span></code></pre></div></div><p><strong>Warning: This is honestly an abuse of swap and should not be regularly relied upon unless you are willing to
drastically shorten your disk&rsquo;s lifespan.</strong></p><p>Afterward, you should be able to load the larger models without much trouble. Even on my incredibly cheap laptop, I am
able to get 2-3 tokens/s (~100-150 words per minute) on 9B and 14B parameter models.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><p>This is extremely impressive for a machine that typically only has 5-6 GB of spare RAM!</p><figure><img class="my-0 rounded-md center-figure" src=/run-llms-larger-than-ram/local-llm-examples.png alt='Two lengthy text responses from LLMs answering the prompt "Hi! Please give a brief overview of what LLMs are and
how they work." They run at ~0.5-0.7 tokens/s during prompt evaluation and ~2.5-3.2 tokens/s during inference.'><figcaption>Sample LLM responses from Google&rsquo;s Gemma2 (left) and Alibaba&rsquo;s Qwen2.5 (right), demonstrating performance at
~0.5-0.7 tokens/s during prompt evaluation and ~2.5-3.2 tokens/s during inference.</figcaption></figure><p><strong>EDIT (February 2025):</strong> Indeed, some users have successfully run the <em>full</em> 671B parameter Deepseek-R1 model off their
SSDs, running at ~40-70 words per minute with under 64 GB of RAM! This is partially thanks to its mixture of experts
(MoE) architecture where only ~37B parameters are activated during each forward pass, making it surprisingly well suited
to this use case.</p><p>Obviously, this comes with the downside of worse performance since the system
is <a href=https://en.wikipedia.org/wiki/Thrashing_%28computer_science%29 target=_blank rel=noreferrer>thrashing</a>. <em>Each</em> forward pass of the neural network
involves shuffling data from disk to RAM and back.<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></p><p>Simply put, if you need to effectively run an extra 1 GB of file transfers for every word the model generates, that
takes a <em>long</em> time compared to having the model fit entirely in RAM.</p><figure><img class="my-0 rounded-md center-figure" src=/run-llms-larger-than-ram/htop-during-prompt-evaluation.png alt="A readout of CPU usage showing ~25% normal usage (green) and ~25% kernel usage (red) across 8 cores."><figcaption>CPU usage during LLM inference with memory thrashing, showing relatively low overall usage but high kernel
activity (in red) due to frequent paging between RAM and disk.</figcaption></figure><p>As such, this can be useful in a pinch for experimentation, but you can only really scale this up as much as your
patience would allow. Running significant portions of the model off disk necessarily means running the whole process
several orders of magnitude slower than usual.</p><h2 class="relative group">See also and references<div id=see-also-and-references class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#see-also-and-references aria-label=Anchor>#</a></span></h2><ul><li>Mozilla&rsquo;s <a href=https://github.com/Mozilla-Ocho/llamafile target=_blank rel=noreferrer>llamafile</a> is probably the easiest way to get started with local
LLMs. It packages all dependencies and model weights into a single executable, so it just takes a single click to run
everything.</li><li><a href=https://ollama.com/ target=_blank rel=noreferrer>Ollama</a> is a very popular tool that provides more flexibility and customization for users
comfortable with a small amount of extra setup.</li><li><a href=https://www.reddit.com/r/LocalLLaMA/ target=_blank rel=noreferrer>/r/LocalLLaMA</a>, a Reddit community focused on local LLM usage, news, and
experiences related to running LLMs on personal hardware.</li><li>The Wikipedia article on <a href=https://en.wikipedia.org/wiki/Thrashing_%28computer_science%29 target=_blank rel=noreferrer>thrashing</a>, which is the term
for this situation where we are constantly paging memory back and forth to disk. This is part of a broader memory
management technique called <a href=https://en.wikipedia.org/wiki/Virtual_memory target=_blank rel=noreferrer>virtual memory</a>.</li><li>A more in-depth <a href=https://github.com/ragibson/linux-page-fault-experiment target=_blank rel=noreferrer>experiment of mine</a> from university on
analyzing page faults through a custom Linux kernel module.</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>While 14 billion parameters might seem absurdly massive for most applications, this is indeed &ldquo;moderately-sized&rdquo;
in the world of consumer-grade LLMs. Many users work with models in the 70B parameter range, which require at least 40
GB of spare memory across system RAM and GPU VRAM.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Technically, this machine has 7 GB of usable system RAM since 1 GB is reserved for the APU, but it doesn&rsquo;t matter
much here. The key part is that this is a very low-end, secondary computer.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>To remove the swap file, you can simply run <code>sudo swapoff /extra_swapfile</code> and delete it. To make the swap
file persistent across boots, you&rsquo;d add an entry into <code>/etc/fstab</code>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Here, I&rsquo;m using a slightly quantized version of Qwen2.5:14b, meaning the model size has been trimmed and reduced
through various lossy optimizations. However, I was still able to run the default version at a <em>much</em> slower rate.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>CPUs generally cannot operate directly on data stored in swap directly. So, when the program needs data stored on
disk, it has to first move some data from memory to disk, and then load the required data from disk back into memory.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><h2 class="mt-8 text-2xl font-extrabold mb-10">Related</h2><section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3"><article class="article-link--related relative min-h-full min-w-full overflow-hidden rounded-lg border border-neutral-300 dark:border-neutral-600"><div class="flex-none relative overflow-hidden thumbnail_card_related"><img src=/posts/llm-judge-leaderboard/feature.webp role=presentation loading=lazy decoding=async fetchpriority=low class="not-prose absolute inset-0 w-full h-full object-cover"></div><div class=p-4><header><a href=/posts/llm-judge-leaderboard/ class="not-prose before:absolute before:inset-0 decoration-primary-500 dark:text-neutral text-xl font-bold text-neutral-800 hover:underline hover:underline-offset-2"><h2>Leaderboard of Open LLMs Ranked by LLM Judges</h2></a></header><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-10-15T22:56:20-04:00>15 October 2024</time><span class="px-2 text-primary-500">&#183;</span>
<span title="Reading time">5 mins</span></div><div class="flex flex-row flex-wrap items-center"><a class="relative mt-[0.5rem] me-2" href=/tags/machine-learning/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Machine-Learning
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/large-language-models/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large-Language-Models
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/leaderboard/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Leaderboard
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/evaluation/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Evaluation</span></span></a></div></div><div class="article-link__summary prose dark:prose-invert mt-1 line-clamp-5">An evaluation of recent consumer-grade open LLMs based on ratings generated through an LLM-as-a-judge framework.</div></div><div class="px-6 pt-4 pb-2"></div></article><article class="article-link--related relative min-h-full min-w-full overflow-hidden rounded-lg border border-neutral-300 dark:border-neutral-600"><div class="flex-none relative overflow-hidden thumbnail_card_related"><img src=/extra/evaluating-llms-with-llm-judges/feature.webp role=presentation loading=lazy decoding=async fetchpriority=low class="not-prose absolute inset-0 w-full h-full object-cover"></div><div class=p-4><header><a href=/extra/evaluating-llms-with-llm-judges/ class="not-prose before:absolute before:inset-0 decoration-primary-500 dark:text-neutral text-xl font-bold text-neutral-800 hover:underline hover:underline-offset-2"><h2>Evaluating LLM Performance via LLM Judges</h2></a></header><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-10-15T20:36:02-04:00>15 October 2024</time><span class="px-2 text-primary-500">&#183;</span>
<span title="Reading time">4 mins</span></div><div class="flex flex-row flex-wrap items-center"><a class="relative mt-[0.5rem] me-2" href=/tags/machine-learning/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Machine-Learning
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/large-language-models/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large-Language-Models
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/evaluation/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Evaluation
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/methodology/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Methodology
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/extra/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Extra</span></span></a></div></div><div class="article-link__summary prose dark:prose-invert mt-1 line-clamp-5">Methodology details for how LLMs can rate the performance of other LLMs.</div></div><div class="px-6 pt-4 pb-2"></div></article><article class="article-link--related relative min-h-full min-w-full overflow-hidden rounded-lg border border-neutral-300 dark:border-neutral-600"><div class="flex-none relative overflow-hidden thumbnail_card_related"><img src=/posts/injected-approval-llm-jailbreak/feature.webp role=presentation loading=lazy decoding=async fetchpriority=low class="not-prose absolute inset-0 w-full h-full object-cover"></div><div class=p-4><header><a href=/posts/injected-approval-llm-jailbreak/ class="not-prose before:absolute before:inset-0 decoration-primary-500 dark:text-neutral text-xl font-bold text-neutral-800 hover:underline hover:underline-offset-2"><h2>Injected Approval: A Low Effort Local LLM Jailbreak</h2></a></header><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-20T12:47:48-05:00>20 December 2024</time><span class="px-2 text-primary-500">&#183;</span>
<span title="Reading time">4 mins</span></div><div class="flex flex-row flex-wrap items-center"><a class="relative mt-[0.5rem] me-2" href=/tags/large-language-models/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large-Language-Models
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/jailbreaking/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Jailbreaking
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/cybersecurity/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Cybersecurity</span></span></a></div></div><div class="article-link__summary prose dark:prose-invert mt-1 line-clamp-5">A quick look into into one of the simplest attacks on LLM safety mitigations, revealing large gaps in current approaches from major tech companies.</div></div><div class="px-6 pt-4 pb-2"></div></article></section></div><script type=text/javascript src=/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA==" data-oid=views_posts/run-llms-larger-than-ram/index.md data-oid-likes=likes_posts/run-llms-larger-than-ram/index.md></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span class="flex flex-col"><a class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" href=/posts/correlation-hedging-risk/><span class=leading-6><span class="inline-block rtl:rotate-180">&larr;</span>&ensp;How Risky is a Correlated Hedge?
</span></a><span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-27T13:07:09-04:00>27 September 2024</time>
</span></span><span class="flex flex-col items-end"><a class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" href=/posts/llm-judge-leaderboard/><span class=leading-6>Leaderboard of Open LLMs Ranked by LLM Judges&ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
</span></a><span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-10-15T22:56:20-04:00>15 October 2024</time></span></span></div></div></footer></article><div id=scroll-to-top class="fixed bottom-6 end-6 z-50 transform translate-y-4 opacity-0 duration-200"><a href=#the-top class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex list-none flex-col sm:flex-row"><li class="flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/tags/ title=Tags>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">Â© 2026 Ryan Gibson</p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500" data-url=https://ryanagibson.com/><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>