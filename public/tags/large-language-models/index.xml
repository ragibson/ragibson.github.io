<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Large-Language-Models on Ryan A. Gibson</title><link>https://ryanagibson.com/tags/large-language-models/</link><description>Recent content in Large-Language-Models on Ryan A. Gibson</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 Ryan Gibson</copyright><lastBuildDate>Fri, 20 Dec 2024 12:47:48 -0500</lastBuildDate><atom:link href="https://ryanagibson.com/tags/large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Injected Approval: A Low Effort Local LLM Jailbreak</title><link>https://ryanagibson.com/posts/injected-approval-llm-jailbreak/</link><pubDate>Fri, 20 Dec 2024 12:47:48 -0500</pubDate><guid>https://ryanagibson.com/posts/injected-approval-llm-jailbreak/</guid><description>A quick look into into one of the simplest attacks on LLM safety mitigations, revealing large gaps in current approaches from major tech companies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://ryanagibson.com/posts/injected-approval-llm-jailbreak/feature.webp"/></item><item><title>Leaderboard of Open LLMs Ranked by LLM Judges</title><link>https://ryanagibson.com/posts/llm-judge-leaderboard/</link><pubDate>Tue, 15 Oct 2024 22:56:20 -0400</pubDate><guid>https://ryanagibson.com/posts/llm-judge-leaderboard/</guid><description>An evaluation of recent consumer-grade open LLMs based on ratings generated through an LLM-as-a-judge framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://ryanagibson.com/posts/llm-judge-leaderboard/feature.webp"/></item><item><title>Evaluating LLM Performance via LLM Judges</title><link>https://ryanagibson.com/extra/evaluating-llms-with-llm-judges/</link><pubDate>Tue, 15 Oct 2024 20:36:02 -0400</pubDate><guid>https://ryanagibson.com/extra/evaluating-llms-with-llm-judges/</guid><description>Methodology details for how LLMs can rate the performance of other LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://ryanagibson.com/extra/evaluating-llms-with-llm-judges/feature.webp"/></item><item><title>How to Run LLMs Larger than RAM</title><link>https://ryanagibson.com/posts/run-llms-larger-than-ram/</link><pubDate>Mon, 30 Sep 2024 21:56:47 -0400</pubDate><guid>https://ryanagibson.com/posts/run-llms-larger-than-ram/</guid><description>A short experiment on running larger LLMs on low-end consumer hardware, with comments on performance trade-offs and practicality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://ryanagibson.com/posts/run-llms-larger-than-ram/feature.webp"/></item></channel></rss>