[{"content":"I pay $15 a month for my phone bill, which grants me unlimited calls, texts, and enough data to get by. It\u0026rsquo;s more than sufficient for my needs and, frankly, a steal compared to the bloated prices of most traditional phone plans.1\nSo, you can imagine my surprise when I logged in one day to discover a long list of mysterious $0.10 charges on my account, all labelled as domestic texting fees. Since there\u0026rsquo;s little information online about this bug, I decided to share a summary of my story below.\nTL;DR: If this happens to you, don\u0026rsquo;t waste time with tier 1 or 2 support \u0026ndash; they can\u0026rsquo;t help. Instead, file an FCC complaint if the issue isn\u0026rsquo;t quickly resolved.\nWhile the U.S. is notorious for its lack of consumer protection laws, phone providers are still required to respond to FCC complaints within 30 days.2 As such, they take these complaints seriously, often giving them much more attention than your average support ticket.\nThe issue # Upon logging in, I noticed that my balance was significantly lower than expected. After some digging (and a bit of web scraping3), I have the following information.\nI had been charged hundreds of individual $0.10 fees, seemingly at random, over a month or so. A closer look revealed that the issue was tied to MMS messages and occurred only when the source or destination location was unknown or undefined. The charges could reoccur for each recipient of the message.\nSome views on the website labelled these as \u0026ldquo;DomesticUsage\u0026rdquo; charges, but there was no clear reason for them, and I couldn\u0026rsquo;t replicate the issue consistently.\nSupport timeline # Raising this issue with T-Mobile prompted the worst journey of customer support I\u0026rsquo;ve probably ever experienced, despite previous positive interactions. Something about this particular issue seemed to have completely broken their internal processes.4\nHere\u0026rsquo;s the timeline, lifted directly from the FCC complaint:\nJune 10 (Phone Call): I reported service issues with receiving MMS messages. Support representatives removed blocking features from my account. Minutes later, I received the first $0.10 erroneous texting charge in my entire account history, though I did not notice this until my next bill.\nJuly 14 (Phone Call): I noticed the overcharges and called T-Mobile. Support identified the fees as account errors and assured me that engineers would investigate and respond within three days. They never did.\nJuly 14 (Email): I followed up via email, verifying on T-Mobile\u0026rsquo;s website that the fees were for texts to domestic recipients and should not have occurred. I further requested a root cause investigation so that the persistent issue stops reoccurring.\nJuly 15 (Email): T-Mobile responded, falsely claiming the fees were for \u0026ldquo;out of plan\u0026rdquo; numbers or chat/conference lines and that I would have been informed of the fees when they occurred. Both claims are blatant lies, especially considering these are texting fees (not phone call fees). I replied with proof from their website showing the texts were to domestic recipients and should not incur fees. T-Mobile did not respond.\nJuly 22 (Phone Call): I called again to follow up. Support re-escalated the issue and assured me that engineers would respond within three days. They further assured me that these fees should not re-occur. Again, engineers never reached out, and the fees continued.\nJuly 26 (Phone Call): I experienced blocked MMS messages and called T-Mobile. Support removed the blocking but failed to address the fee issue. They assured me that engineers would respond within days. They did not, and the fees persisted.\nJuly 26 (Email): I reached out again to complain about the support quality and reiterated that the fees were due to a T-Mobile error. T-Mobile did not respond.\nAs you can tell, I was understandably frustrated at the time of writing.\nMy FCC complaint was served to T-Mobile on August 1st. About a week later, I received a call from their \u0026ldquo;Executive Response Team\u0026rdquo;.\nExecutive response # The quality of the executive support was immediately phenomenal, and they seemed just as puzzled by the situation as I was.\nSurprisingly, the executive team is so far removed from the prepaid side of the business that they were essentially debugging the issue alongside me:\nThey initially suspected that MMS attachments might incur extra fees. They consulted with several of their coworkers but couldn\u0026rsquo;t determine if this is expected behavior \u0026ndash; prepaid service being \u0026ldquo;no contract\u0026rdquo; understandably complicates things.5 They also struggled to reconcile the charges. Their internal systems didn\u0026rsquo;t match up with the amounts I was actually charged, causing significant confusion. Funnily enough, they were reviewing internal reports that weren\u0026rsquo;t client-facing, and they couldn\u0026rsquo;t access the reports I was viewing on their own website. Moreover, their systems only preserved complete logs for 72 hours, making it impossible to drill down to the root cause unless the issue reoccurred. Eventually, we reviewed a spreadsheet I had compiled for the FCC complaint, and they accepted my analysis. My understanding matched most of their internal data, so they credited my account for all the erroneous fees.\nThe issue has not reoccurred for about a month, so it seems to have been resolved somehow.\nMy theories # Since support was never able to figure out why this occurs, I have a few theories.\nRandom Pay-As-You-Go Glitch: Their system might sporadically treat texts as if they were on a pay-as-you-go plan. There are similar complaints online from around the same time, so it\u0026rsquo;s possible that engineers quietly fixed the issue without informing anyone or addressing past charges.\nAccidental Roaming Fees: Perhaps they\u0026rsquo;re mistakenly charging roaming fees from domestic roaming or peering agreements with other providers. These should be free, but without contractual details, it\u0026rsquo;s anyone\u0026rsquo;s guess.\nBotched Porting Procedures: T-Mobile\u0026rsquo;s porting procedures might be broken, and an earlier transfer of mine was improperly processed.6 If so, texts could be routed as if they were for a non-existent postpaid account, prompting the charges.\nThis would also explain sporadic MMS issues I\u0026rsquo;ve experienced over the past few months, with messages occasionally being dropped or blocked with cryptic errors.\nT-Mobile\u0026rsquo;s prepaid and postpaid sides are effectively separate companies due to repeated mergers, but it would be mind-boggling if they were charging themselves for internal peering.\nSee also and references # If you find yourself in a similar situation, start by visiting the FCC\u0026rsquo;s FAQ page on filing a complaint.\nYou can also review these online threads discussing similar issues:\n\u0026ldquo;Getting random charges for MMS on my Prepaid plan (unlimited 3gb)\u0026rdquo; (December 2016). \u0026ldquo;Tmobile Connect Plan- Random 10 cents taken out of prepaid balance?\u0026rdquo; (September 2020) \u0026ldquo;Random 0.10 charges\u0026rdquo; (May 2024) \u0026ldquo;My mom has a legacy pay-as-you-go (prepaid) plan, but she\u0026rsquo;s getting charged at least $0.20 every day. Why?\u0026rdquo; (July 2023) \u0026ldquo;Legacy Pay As You Go Mysterious Random Outgoing SMS Messaging Charges (May 2024)\u0026rdquo; In fact, I would go so far as to say that most traditional phone plans are a scam. The cheapest Verizon plan is $75/month for a single line with zero mobile hotspot data ($65/month with auto-pay). That\u0026rsquo;s five times what I pay!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nConsumers typically go through the \u0026ldquo;informal\u0026rdquo; complaint process, which can be completed online for free. A formal complaint invokes something more akin to court proceedings, has a filing fee of \u0026gt;$600, and usually requires legal representation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSurprisingly, web scraping was required because the fees were zeroed out in the website\u0026rsquo;s exports and downloads. Other reports appeared incomplete and/or out of chronological order.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI accept that prepaid service comes with lower profit margins, and thus support quality, but this experience was nigh unbelievable.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTo be fair to them, the only details on their prepaid plan state:\nThis plan does not include international roaming, calling, or messaging. [\u0026hellip;]\nUnlimited talk \u0026amp; text features for basic direct communications between 2 people; others (e.g., conference, chat lines, iMessage) may cost extra or require data allotment.\nThis appears to be a generic disclaimer that some texting-like features actually consume data, but it could be stretched to justify charges for group chats or MMS messages.\nThat said, if they were to act on this and retroactively justify the charges under this clause, it would almost certainly constitute fraud.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIndeed, this appears to be a separate, ongoing issue for the company. See \u0026ldquo;PSA to anybody who’s ported out of T-Mobile prepaid to a non T-Mobile carrier in the past few weeks\u0026rdquo; (March 2024) and \u0026ldquo;Left T-mobile prepaid for US mobile. Now I can\u0026rsquo;t receive MMS from anyone using T-Mobile\u0026rdquo; (March 2024).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"26 August 2024","externalUrl":null,"permalink":"/posts/t-mobile-prepaid-billing-errors/","section":"Posts","summary":"A discussion of an odd bug in T-Mobile\u0026rsquo;s systems, resolved only after filing an FCC complaint. This post details my experience, the support timeline, and some theories on the mysterious charges.","title":"A Tale of T-Mobile Prepaid Billing Errors","type":"posts"},{"content":"","date":"26 August 2024","externalUrl":null,"permalink":"/tags/consumer-protection/","section":"Tags","summary":"","title":"Consumer-Protection","type":"tags"},{"content":"","date":"26 August 2024","externalUrl":null,"permalink":"/tags/fcc-complaint/","section":"Tags","summary":"","title":"FCC-Complaint","type":"tags"},{"content":"","date":"26 August 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"26 August 2024","externalUrl":null,"permalink":"/tags/prepaid-plans/","section":"Tags","summary":"","title":"Prepaid-Plans","type":"tags"},{"content":"","date":"26 August 2024","externalUrl":null,"permalink":"/","section":"Ryan A. Gibson","summary":"","title":"Ryan A. Gibson","type":"page"},{"content":"","date":"26 August 2024","externalUrl":null,"permalink":"/tags/t-mobile/","section":"Tags","summary":"","title":"T-Mobile","type":"tags"},{"content":"","date":"26 August 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"29 July 2024","externalUrl":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":"","date":"29 July 2024","externalUrl":null,"permalink":"/tags/programming/","section":"Tags","summary":"","title":"Programming","type":"tags"},{"content":"","date":"29 July 2024","externalUrl":null,"permalink":"/tags/repository-management/","section":"Tags","summary":"","title":"Repository-Management","type":"tags"},{"content":"","date":"29 July 2024","externalUrl":null,"permalink":"/tags/version-control/","section":"Tags","summary":"","title":"Version-Control","type":"tags"},{"content":"It is bad practice to commit large data files to version control unless absolutely necessary. However, many novices treat Git more like a general storage solution than a version control system.\nRemember: if you commit something, it\u0026rsquo;s bloating the repository forever!\nGit must effectively retain every version of every file that has ever existed in your repository.1 This is a fundamental feature of version control, allowing recovery of the full history of changes.\nTo illustrate the issue, let\u0026rsquo;s look at one of the more popular Hugo themes, Blowfish.\nBlowfish has the largest Git repository out of all the major Hugo themes, by far. As you can see, Blowfish\u0026rsquo;s repository is ~25x to ~65x larger than the median, depending on whether you\u0026rsquo;re considering the most recent version or the entire commit history.2 This has understandably led to a number of user complaints.\nMost projects should be quite small and rarely need to exceed a few hundred MB in their Git repositories.3\nWhat went wrong with Blowfish? # To understand the problem, let\u0026rsquo;s see what\u0026rsquo;s in this repository.\n$ ncdu ~/Desktop/blowfish --- ~/Desktop/blowfish ------------------ 450.1 MiB [##########] /.git 48.1 MiB [# ] /exampleSite 6.8 MiB [ ] /assets 6.2 MiB [ ] /images 544.0 KiB [ ] /layouts ... Incredibly, the author has committed the entire example site to the theme itself. Unsurprisingly, this includes the largest files in the Git tree, with images taking up a significant portion.\n$ git ls-tree -r -t -l --full-name HEAD | sort -n -k 4 -r | head -n 5 100644 blob 9f1220a6b0f2c87acd0f52187f905c4a12d7547c 5097493\texampleSite/assets/img/ocean.jpg 100644 blob 6352a781b3a41106cf3cf7a1175692d5009410a2 5000159\texampleSite/assets/img/iceland.jpg 100644 blob 946d6ae539f70cbee1a0ebe939ba0beb75fc619e 3715726\timages/home-card.png 100644 blob 3dce007acabbec440d303bb632042981d5354e96 3335717\tassets/lib/mermaid/mermaid.min.js 100644 blob 2455b8158a66e80544dbb23da982826e5a8b5a37 2360966\texampleSite/content/guides/202310-blowfish-tutorial/img/01.png In fact, 90% of the example site\u0026rsquo;s file size is composed of images.\n$ find exampleSite/ -regex \u0026#39;.*\\.\\(png\\|jpg\\)\u0026#39; -exec stat -c %s {} \\; | awk \u0026#39;{sum+=$1}END{print sum}\u0026#39; 43200010 # ~43 MB of the ~48 MB exampleSite/ directory Moreover, the author has been committing a screenshot of every site that uses the theme! Needless to say, this is horrifically bad practice as it forces every user to download these files to use the theme.4\nEvery user has a screenshot of their site committed to Blowfish on request. A glance through a simple analysis from the git-filter-repo command reveals even more issues.\nIn the past, they have accidentally committed non-minified versions of JavaScript dependencies, added an entire node_modules/ folder of third-party libraries, pushed several complete builds of since-deleted example sites, and regularly committed large images that are unused in the theme itself.\n$ git filter-repo --analyze $ less .git/filter-repo/analysis/path-all-sizes.txt === All paths by reverse accumulated size === Format: unpacked size, packed size, date deleted, path name ... 6942334 6943896 2023-10-15 public/docs/welcome/featured.png ... 6600126 6592756 \u0026lt;present\u0026gt; images/screenshot.png ... 5000159 4997680 2022-10-02 docs/iceland.jpg ... 14460503 1336164 2024-07-02 assets/lib/mermaid/mermaid.js $ less .git/filter-repo/analysis/directories-deleted-sizes.txt === Deleted directories by reverse size === Format: unpacked size, packed size, date deleted, directory name 174077004 166873965 2023-10-15 public ... 100530759 58273713 2022-10-02 docs ... 30062914 29178879 2022-09-11 exampleSitePersonal ... 29283034 28540711 2022-09-10 content How to fix it # Clearly, there are a lot of issues in this repository, so let\u0026rsquo;s try to fix them.\nThe methods available will depend on whether we have full write access to the repo as a maintainer or are simply a user.\nAs a user # As a user of the theme,\nYou probably don\u0026rsquo;t need the history of the repository, so using a shallow clone is a good start. You definitely don\u0026rsquo;t need the example site and related images, so removing them with a sparse checkout will help significantly. By cloning only the most recent commit and excluding unnecessary paths, we can greatly reduce the size of the download and disk usage.\ngit clone --filter=blob:none --no-checkout --depth=1 --sparse https://github.com/nunocoracao/blowfish.git cd blowfish printf \u0026#39;/*\\n!exampleSite/*\\n!images/*\\n!assets/img/*\\n!*.png\u0026#39; \u0026gt; .git/info/sparse-checkout git checkout This approach immediately gets the clone down to a \u0026lt;3 MB download and ~9 MB of total disk usage, which should be suitable for most purposes.\nFor the sparse checkout, we are only interested in the following paths:\n/* # we want everything, except !exampleSite/* # nothing from exampleSite/ !images/* # nothing from images/ !assets/img/* # nothing from assets/img/ !*.png # and no PNGs from the root directory Additionally, much of the remaining large files are bundled flowchart and math typesetting libraries, mermaid and katex. If those are not required for your use case, removing them further reduces the overall theme size to ~2 MB!\nAs a maintainer # As a maintainer with write access to the repository, we\u0026rsquo;ll need to go back in time and retroactively remove all these unnecessary files and directories. We\u0026rsquo;ll use the git-filter-repo command to accomplish this.\nWarning: rewriting Git history is destructive. Make sure you have local backups before proceeding. After making these changes, all collaborators should start with a fresh clone to prevent conflicts and avoid merging away all your work.\nWe already know that we need to clean up exampleSite/, images/, and assets/img/. We should also eliminate the public/, docs/, exampleSitePersonal/, content/, and node_modules/ directories that were present in the history but not in recent commits.\n$ git filter-repo --invert-paths --path exampleSite/ --path images/ --path assets/img/ --path public/ --path docs/ --path exampleSitePersonal/ --path content/ --path node_modules/ $ du -s --si 37M\t. Applying this filter command reduces the repository size from ~534 MB to ~37 MB \u0026ndash; a significant improvement. However, to further address the bloat, we\u0026rsquo;ll need to dig deeper.\n$ git filter-repo --analyze $ head -n 10 .git/filter-repo/analysis/path-all-sizes.txt === All paths by reverse accumulated size === Format: unpacked size, packed size, date deleted, path name 13659165 2684178 \u0026lt;present\u0026gt; assets/lib/mermaid/mermaid.min.js 42913700 1754245 \u0026lt;present\u0026gt; package-lock.json 14460503 1336163 2024-07-02 assets/lib/mermaid/mermaid.js 15266445 1238056 \u0026lt;present\u0026gt; assets/css/compiled/main.css 4564827 1047141 2024-07-02 assets/lib/mermaid/flowchart-elk-definition-a7fe3362.js.map 4564827 1047140 2024-07-02 assets/lib/mermaid/flowchart-elk-definition-2f51e52a.js.map 3306156 918888 \u0026lt;present\u0026gt; assets/lib/tw-elements/index.min.js 4121397 702693 2024-07-02 assets/lib/mermaid/flowchart-elk-definition-6f3d7532.js.map This analysis reveals some non-minified JavaScript dependencies for the mermaid library. The author has accidentally committed both minified and non-minified version in the past.\nThe most recent version of the theme only needs the minified assets/lib/mermaid/mermaid.min.js, so we can remove the rest.\n$ ls assets/lib/mermaid/ mermaid.min.js $ git filter-repo --invert-paths --path-regex \u0026#39;^assets\\/lib\\/mermaid\\/(?!mermaid\\.min\\.js$).*\u0026#39; $ du -s --si 20M\t. With these additional files filtered out, our local repository size reduces to ~20 MB.\nAfter pushing these changes back to GitHub, a shallow clone takes ~3MB of download and ~10MB of total disk usage. Including the full Git history requires just ~18MB, which is a substantial reduction.\nOverall, we\u0026rsquo;ve managed to trim the Git history down to just 3.4% of its original size without sacrificing functionality or version control. Even a shallow clone of just the most recent commit is ~8.6% the size of the upstream repository.\nSee also and references # The documentation of git-filter-repo, the tool recommended by the Git project for efficiently rewriting large chunks of Git history. GitHub\u0026rsquo;s documentation on rewriting history to remove sensitive data. This is additionally useful for understanding the broader context of history rewriting as it pertains to data security. The Git documentation on shallow clones and sparse checkouts. To be clear, Git doesn\u0026rsquo;t store full copies of every version or file. Common components are shared and compressed whenever possible. However, the core lesson remains: adding large files will permanently affect your users.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA default (full) git clone takes \u0026gt;500MB of disk space and cloning just the most recent commit with the --depth=1 flag takes \u0026gt;100MB. The .git/ version control information alone accounts for ~470 MB of a full clone and ~50 MB of a shallow one.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAn analysis of the top 100 most starred projects on GitHub shows that the median repository only uses ~90 MB of disk space for a full clone.\nThis analysis includes the most popular libraries in the world, such as React, TensorFlow, Bootstrap, Visual Studio Code, the Go programming language, Electron, Kubernetes, Node.js, and many more.\nIn fact, the vast majority of these projects remain under 1 GB, which aligns with GitHub\u0026rsquo;s own recommendation for ideal repository size. GitHub also strongly recommends keeping repositories under 5 GB, a limit only breached by the Linux kernel here, with its 1.3 million commits and \u0026gt;19 years of history. Your typical project should never reach these sizes without an extremely good reason.\n\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is true by default, but we\u0026rsquo;ll see how to selectively ignore these files later via a Git sparse checkout.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"29 July 2024","externalUrl":null,"permalink":"/posts/shrink-git-repo/","section":"Posts","summary":"A guide on reducing the size of an oversized Git repository, using the Blowfish Hugo theme as a case study. Learn best practices for maintaining a lean repo and its history.","title":"Why the Hell Is This Git Repo So Large? And How to Trim It Down","type":"posts"},{"content":" A few months ago, a job recruiter warned me that a job listing was in a \u0026ldquo;red state,\u0026rdquo; eventually clarifying that an increasing number of applicants were refusing to consider moving to states with a strong Republican lean.\nWhile there are numerous valid personal reasons1 for this, the bluntness surprised me and made me curious precisely how much quality of life varies with the political leaning of a state.\nTo be clear, none of the conclusions here prove a causal relationship. The focus is on a broad comparison based on the recruiter\u0026rsquo;s simple \u0026ldquo;red state vs. blue state\u0026rdquo; framework.\nSignificant trends, in order of statistical significance # When values are aggregated from lower-level metrics, we split the hierarchy into high-level, mid-level, and low-level metrics. This hierarchy helps us understand the broader trends while also allowing us to zoom in on specific areas of interest.\nBelow, we present the strength of the statistically significant2 trends as explained by political party lean alone. Later, we\u0026rsquo;ll show plots that demonstrate a large fraction of these trends remain significant even after correcting for differences in state wealth (measured by per capita GDP) and population density.3\n(Click to zoom) This plot shows a strong negative correlation between Republican voting patterns and public health metrics, even after adjusting for GDP and population density. High-level metrics # Overall, Republican states:\nHave stronger government fiscal stability, such as a balanced budget \\( \\left( p \\approx 0.1\\% \\right) \\) Offer more opportunity, such as lower cost of living \\( \\left( p \\approx 0.2\\% \\right) \\) On the other hand, Democratic states:\nHave better health care \\( \\left( p \\approx 1 \\cdot 10^{-11} \\right) \\) Enjoy higher incomes \\( \\left( p \\approx 6 \\cdot 10^{-9} \\right) \\) Live longer \\( \\left( p \\approx 2 \\cdot 10^{-6} \\right) \\) Are happier \\( \\left( p \\approx 0.02\\% \\right) \\) Have better environments, such as less pollution \\( \\left( p \\approx 0.7\\% \\right) \\) Enjoy more personal freedoms4 \\( \\left( p \\approx 0.9\\% \\right) \\) Mid-level metrics # Overall, Republican states:\nAre more affordable \\( \\left( p \\approx 2 \\cdot 10^{-13} \\right) \\) Demonstrate better short-term government fiscal stability \\( \\left( p \\approx 0.01\\% \\right) \\) While Democratic states:\nHave better public health \\( \\left( p \\approx 5 \\cdot 10^{-10} \\right) \\) Enjoy higher emotional and physical well-being \\( \\left( p \\approx 3 \\cdot 10^{-6} \\right) \\) Provide broader health care access \\( \\left( p \\approx 1 \\cdot 10^{-5} \\right) \\) Achieve better crime / corrections outcomes \\( \\left( p \\approx 1 \\cdot 10^{-5} \\right) \\) Offer higher economic opportunity \\( \\left( p \\approx 0.07\\% \\right) \\) Maintain better health care quality \\( \\left( p \\approx 0.08\\% \\right) \\) Foster better business environments \\( \\left( p \\approx 0.1\\% \\right) \\) Achieve higher equality \\( \\left( p \\approx 0.7\\% \\right) \\) Ensure less pollution \\( \\left( p \\approx 1.0\\% \\right) \\) Selected low-level metrics # There are too many significant trends to list here, so we only highlight those with minimal overlap with previous higher-level categories. All trends can be viewed in the dropdown selector in the next section.\nOverall, Republican states:\nEnjoy shorter commutes \\( \\left( p \\approx 1 \\cdot 10^{-5} \\right) \\) Have a lower tax burden \\( \\left( p \\approx 3 \\cdot 10^{-5} \\right) \\) Maintain higher-quality roads \\( \\left( p \\approx 6 \\cdot 10^{-5} \\right) \\) Earn more 2-year college degrees \\( \\left( p \\approx 0.1\\% \\right) \\) Experience higher migration rates \\( \\left( p \\approx 1.3\\% \\right) \\) In contrast, Democratic states:\nHave better public transit \\( \\left( p \\approx 1 \\cdot 10^{-10} \\right) \\) Achieve more equal incomes between genders \\( \\left( p \\approx 2 \\cdot 10^{-8} \\right) \\) Are more educated \\( \\left( p \\approx 7 \\cdot 10^{-8} \\right) \\) Maintain lower incarceration rates \\( \\left( p \\approx 3 \\cdot 10^{-7} \\right) \\) Ensure better internet access \\( \\left( p \\approx 2 \\cdot 10^{-6} \\right) \\) Have lower suicide rates \\( \\left( p \\approx 5 \\cdot 10^{-6} \\right) \\) Create more patents \\( \\left( p \\approx 1 \\cdot 10^{-5} \\right) \\) Earn more 4-year college degrees \\( \\left( p \\approx 0.02\\% \\right) \\) Receive more venture capital funding for businesses \\( \\left( p \\approx 0.02\\% \\right) \\) Ensure lower food insecurity and lower poverty rates \\( \\left( \\text{both } p \\approx 0.04\\% \\right) \\) Enjoy more affordable health care \\( \\left( p \\approx 0.05\\% \\right) \\) Host more company headquarters \\( \\left( p \\approx 0.2\\% \\right) \\) Maintain more reliable power grids \\( \\left( p \\approx 2.1\\% \\right) \\) Plots of all trends (dropdown selection) # Here, we provide a hierarchical view of every plot. Use the dropdown menu to explore various quality of life metrics and see how they correlate with political party strength.\nThe WalletHub (\u0026ldquo;WH\u0026rdquo;) happiness scores have two layers of hierarchical information, while the US News (\u0026ldquo;USN\u0026rdquo;) quality of life categories have three layers of metrics.\nThe 16 metrics that are significantly better in Republican states are marked \u0026ldquo;[R]\u0026rdquo; and the 43 metrics that are significantly better in Democratic states are marked \u0026ldquo;[D]\u0026rdquo;.\nSelect metric to plotHigh-level: N/A - is JavaScript disabled?Mid-level: N/A - is JavaScript disabled?Low-level: N/A - is JavaScript disabled? (Click to zoom) Left: Plot of a quality of life metric versus the 2022 Cook Partisan Voting Index, as selected by the dropdowns above. Right: a plot of the same data after removing the portion of the overall trend that can be explained by differences in state per capita GDP and population density. Statistically significant trends are denoted by a black dashed line. See also and references # For additional details, see the links in the footnotes and the data sets themselves. I primarily analyzed data from the following sources:\nThe 2022 Cook Partisan Voter Index, which judges the political lean of each state over the last two presidential elections compared to the nation as a whole. They only publicly publish this to whole percentage points, so I replicated the calculation5 myself for additional precision using the 2016 and 2020 publications from the Federal Election Commission.6 The Cato Institute\u0026rsquo;s 2023 Personal Freedom Index The 2024 US News Best States Rankings at all levels of their hierarchy: (high-level) categories, (mid-level) category attributes, and (low-level) category attribute metrics. WalletHub\u0026rsquo;s 2023 Happiest States in America The U.S. Bureau of Labor Statistics\u0026rsquo;s May 2023 Occupational Employment and Wage Estimates The U.S. Bureau of Economic Analysis\u0026rsquo;s 2023 state GDPs The U.S. Census Bureau\u0026rsquo;s 2023 state population estimates and 2010 State Area Measurements for population density calculations The U.S. Centers for Disease Control and Prevention 2020 States Life Tables Reproductive and LGBT rights immediately come to mind as unequivocal deal-breakers for many individuals. Similarly, I know a number of teachers who refuse to consider Republican states due to increasing attacks on education in general.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHere and in the later plots, we\u0026rsquo;re using a significance level of \\( \\alpha = 0.05 \\), Benjamini-Hochberg corrected for the large number of multiple comparisons. This effectively controls the false discovery rate without inflating the likelihood of false positives and ends up being equivalent to taking a per-trend significance level of \\( \\alpha \\approx 2.1\\% \\).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlmost all the metrics analyzed here are already quoted on a per-capita basis, so correcting for population density effects is more appropriate than population alone. Technically speaking, we actually correct against log population density in order to address heteroscedasticity in the data.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt might be surprising on the surface that Republican states have significantly fewer personal freedoms than Democratic ones, but as the Cato Institute explains, \u0026ldquo;socially conservative states tend to restrict alcohol, gambling, marijuana, and, until Obergefell v. Hodges, marriage freedoms.\u0026rdquo; Other examples include some GOP charters striving to require proof of fault for all divorces and aiming to repeal the Voting Rights Act of 1965.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis was actually somewhat difficult since their official methodology and every source I could find are ambiguous and/or misleading on the precise calculation. What they\u0026rsquo;re actually doing is\nTabulate per-state vote counts for the Republican and Democratic candidates. Calculate the percentage that voted Republican/Democratic out of those that voted for either party. So, a state that votes 45% Republican / 50% Democratic gets normalized to ~47.4% Republican / ~52.6% Democratic. Compare this to the same nation-wide calculation, again normalizing the percentage by ignoring all third-party voters. This uses the total votes for each party\u0026rsquo;s candidate across the entire country rather than an aggregation of the states\u0026rsquo; results. I.e., the comparison is to the \u0026ldquo;average voter\u0026rdquo;, not the \u0026ldquo;average state\u0026rdquo;. Subtract the per-state and nation-wide party leans and report the final index as the 75% / 25% weighted average of the last two presidential elections. \u0026#160;\u0026#x21a9;\u0026#xfe0e; As a humorous aside, the official U.S. Excel export for the 2020 results have an overlooked note to double-check one of the vote counts in Connecticut (placed in a column that should be blank for that state). It seems they officially tabulated 219 votes for some minor candidate, but wanted to double-check if the actual count was 218? Indeed, the government site for the Connecticut Secretary of State claims 218 votes here, so the discrepancy appears real. \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"16 July 2024","externalUrl":null,"permalink":"/posts/political-party-vs-quality-of-life/","section":"Posts","summary":"A data-driven exploration of how political lean correlates with quality of life metrics across the United States.","title":"Analyzing Political Party Strength vs. Quality of Life in U.S. States","type":"posts"},{"content":"","date":"16 July 2024","externalUrl":null,"permalink":"/tags/data-analysis/","section":"Tags","summary":"","title":"Data-Analysis","type":"tags"},{"content":"","date":"16 July 2024","externalUrl":null,"permalink":"/tags/data-exploration/","section":"Tags","summary":"","title":"Data-Exploration","type":"tags"},{"content":"","date":"16 July 2024","externalUrl":null,"permalink":"/tags/interactive/","section":"Tags","summary":"","title":"Interactive","type":"tags"},{"content":"","date":"16 July 2024","externalUrl":null,"permalink":"/tags/politics/","section":"Tags","summary":"","title":"Politics","type":"tags"},{"content":"","date":"16 July 2024","externalUrl":null,"permalink":"/tags/quality-of-life/","section":"Tags","summary":"","title":"Quality-of-Life","type":"tags"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/extra/","section":"Tags","summary":"","title":"Extra","type":"tags"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/extra/","section":"Extra","summary":"","title":"Extra","type":"extra"},{"content":"This is an extra post accompanying \u0026ldquo;Modeling Optimal PC Building Decisions\u0026rdquo;, which analyzed historical data to construct a model of computer performance over time and optimize budget choices.\nMinor details omitted from main post # For performance calculations, we used a 70% performance boost in cases where older builds had an SLI GPU configuration.\nAs mentioned in the main post, the actual improvement depends heavily on the use case, but this is a decent approximation and matches legacy guidance from Logical Increments themselves.\nRaw data sets # Here, we store the raw data sets that we used.\nCPU/GPU benchmarks # See cpumark_20240222.csv and gpumark_20240222.csv, which contain complete sets of CPU and GPU benchmarks scraped from PassMark in February 2024.\nLogical Increments PC build history # See logicalincrements_history_2013_to_2024.csv, containing historical PC builds for the 10 tiers provided by Logical Increments from 2013 to 2024, scraped with help from the Internet Archive.\nTo be precise, the data ranges from December 29, 2012, to April 1, 2024, with samples taken approximately every three months.\nFor each build, we include the component names and prices for the GPU, CPU, motherboard, and RAM, as well as the total build cost. Interestingly, some of the older builds had their totals calculated incorrectly in the website, so I\u0026rsquo;ve corrected those discrepancies here.\nMapping between Logical Increments and PassMark component names # Importantly, the Logical Increments data uses shorthands for the component names, so we needed to map them to the more precise names in the PassMark benchmarks. This was overwhelmingly an automated process,1 but we\u0026rsquo;ve replicated the final mapping in logicalincrements_passmark_component_mapping.csv.\nNote that SLI setups are mapped to their single-GPU equivalents (e.g., both \u0026ldquo;GTX 770\u0026rdquo; and \u0026ldquo;GTX 770 x2\u0026rdquo; are mapped to \u0026ldquo;GeForce GTX 770\u0026rdquo;).\nInflation data # For inflation computations, we used the Core CPI, CPILFESL2, as available from the Federal Reserve Economic Data (FRED).\nThere are two mappings here that are close approximations since the components do not exist in PassMark. The \u0026ldquo;Ryzen 5 1600AF\u0026rdquo; (which is not the Ryzen 5 1600) is mapped to the Ryzen 5 2600 and the \u0026ldquo;Radeon R7 265\u0026rdquo; (which is not the Radeon R7 M265) is mapped to the Radeon RX 460.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRoughly speaking, this stands for the Consumer Price Index Less Food and Energy, Seasonally adjusted Level.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"20 May 2024","externalUrl":null,"permalink":"/extra/historical-pc-build-data/","section":"Extra","summary":"Historical data sets I\u0026rsquo;ve scraped on PC build composition and performance benchmarks.","title":"Historical PC Build Data","type":"extra"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/historical-data/","section":"Tags","summary":"","title":"Historical-Data","type":"tags"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/pc-building/","section":"Tags","summary":"","title":"Pc-Building","type":"tags"},{"content":"","date":"16 May 2024","externalUrl":null,"permalink":"/tags/modeling/","section":"Tags","summary":"","title":"Modeling","type":"tags"},{"content":" When building out a new computer, it is relatively easy to calculate which choices would be most efficient in terms of performance per dollar.\nBut how quickly will that performance degrade relative to newer systems entering the market? What if you expect to recover some of those funds by selling components once you upgrade? Is there an optimal frequency for upgrading components? Is it possible to \u0026ldquo;future-proof\u0026rdquo; a system?\nThese questions require a deeper analysis into historical data to understand how computer performance changes over time and relates to build cost.\nThe data set # To study the overall state of PC builds through time, we\nScraped the last ~11 years of build recommendations from Logical Increments and adjusted the prices for inflation.1 In general, they provide component suggestions across a wide range of \u0026quot; tiers\u0026quot; (budgets). Pulled a complete set of CPU and GPU benchmarks from PassMark, assigning half weight to both components when evaluating overall system performance.2 Below, we plot the historical trend of build prices and performance across many of the tiers.\nA plot of inflation-adjusted prices for Logical Increments build tiers from 2013 to 2024. Overall, the prices are quite stable across time for most tiers, with the notable exception of the spike during the global chip shortage and cryptocurrency mining boom of 2021.\nSimilarly, build performance shows a steady increase over time, which is depicted below.3\nA plot of system performance for Logical Increments build tiers from 2013 to 2024. More details and the data itself can be found in \u0026ldquo;Historical PC Build Data\u0026rdquo;. Next, we\u0026rsquo;ll derive some model assumptions from this data.\nModel assumptions # Fundamentally, our model aims to project PC performance over time, which will tell us the relative speed of obsolescence between older and newer builds. Separately, we\u0026rsquo;ll fit estimates of build performance against total component cost.\nThis will rely on two basic assumptions.\n1) Computer performance improves at a constant rate, regardless of cost # Our model operates under the assumption that computer performance experiences a consistent rate of improvement over time, regardless of the initial cost of the components. This is supported by the fact that our plots of log-performance show a roughly linear trend across time for all tiers.\nMoreover, it is a reasonable assumption that computer chip fabrication techniques fundamentally improve simultaneously across all consumer cost ranges. Indeed, low-end and mid-end components are often just binned versions of their high-end counterparts.\nOur fit to the data is shown below, and appears quite reasonable.\nA plot of the simultaneous linear fit of PC performance over time. All ten tiers share the same slope, yielding an annual performance growth of ~20.5%. We\u0026rsquo;ll only use the slope (performance growth rate) here. However, it\u0026rsquo;s important to acknowledge a few limitations and nuances.\nThere is considerable noise at lower end of the performance spectrum, partially due to recent GPU supply issues and sporadic significant improvements in cheaper components. There is some tapered growth at the higher end, partly due to the phase-out of SLI setups, which somewhat inflates the performance of older high-end builds.4 Despite this, the overall fit provides a useful rule of thumb, particularly within the Fair to Exceptional tiers. Those tiers span builds that cost around $700 to $2000, which contains practically the entire range of decent performance per dollar.\n2) We can predict computer performance from component cost alone # Our model also assumes that computer performance can be predicted solely from the cost of its components, which is a direct consequence of all tiers improving at the same rate.\nThis is also based on the desire to translate between a build’s price and its corresponding performance.\nImportantly, these performance values are normalized against those from the same time period, allowing for meaningful comparisons across time. In other words,\n100% on the \u0026ldquo;Global Performance Index\u0026rdquo; from the earlier plots is roughly the performance of a mid-end system from 2019.5 100% on the \u0026ldquo;Time-Specific Performance Index\u0026rdquo; in the plots below indicates performance comparable to the average build from the same time period.6 Below, we plot the relationship between performance and price. I encourage you to consider what functional form would work best here. Clearly, it must be monotonic, but what other features of the data should be captured?\nA plot of PC build performance against their inflation-adjusted prices from 2013-2024. To me, it is clear that there is very rapid improvement per dollar for cheaper tiers, and this slows to a heavily heteroskedastic and roughly-linear trend.\nNaturally, I also wanted the function to pass through the origin (as spending zero money yields zero performance), so I settled on a tweaked logistic function with a linear asymptote.\n$$ f(x) = \\frac{a}{1 + b \\cdot \\exp(-c \\cdot x)} + d \\cdot x - \\frac{a}{1+b} $$\nThe plot below displays the result of a least squares fit with this function, providing performance as a function of price. Importantly, the fit was actually applied to log-performance data to address the heteroskedasticity.\nA model fit of PC build performance as a function of inflation-adjusted price. As with the earlier fit, this appears to be quite good overall. However, there\u0026rsquo;s some inconsistency across time in the data for costs exceeding ~$2000, so I would caution interpreting predictions beyond that point.\nModel results # Now, we can explore the modeled answers to the questions that we started with.\nTo do so, we take a simple approach to planning future upgrades.\nThe CPU and GPU are upgraded with a fixed frequency, which necessitates upgrading the motherboard and RAM as well to support potential socket changes. If used components are resold, we assume they yield 2/3rds of the price of an equivalent new component, accounting for overall computing improvements since the original purchase date. All other components, such as storage and power supplies, are assumed to have a longer lifespan and are replaced once per decade. What is the optimal amount to spend on a new build? # Answering this question fundamentally depends on whether you except to recoup any money from future upgrades through the sale of your used components. If not, it is quite common to gift older builds to relatives or repurpose them as secondary systems.\nIf not planning to resell used components # In this scenario, the decision boils down to a straightforward calculation of performance per dollar.\nA plot of PC performance per dollar of a new build in isolation. Anything within $580 to $1100 is pretty good. Unsurprisingly, the optimal price point is right around what we\u0026rsquo;d typically refer to as \u0026ldquo;mid-range\u0026rdquo;.\nHowever, it\u0026rsquo;s noteworthy that a wide range of prices yield quite close to optimal performance per dollar. In fact, pretty much anything other than high-end and enthusiast builds are within 10% of the best cost efficiency possible.\nIf planning to resell used components # To motivate the range of PC upgrading strategies here, let\u0026rsquo;s consider three approaches.\nUpgrading a $500 PC every 2.5 years Upgrading a $1000 PC every 5 years Upgrading a $2000 PC every 10 years Below, we plot the effective performance of all three builds relative to then-current systems.\nComparing these strategies should reveal a few interesting insights.\nThe $1000 build offers more than twice the performance of the $500 build for less than 50% extra cost. The $2000 build\u0026rsquo;s performance falls below the $1000 build after year 5 and both builds after year 7.5. This is likely to be an unacceptable outcome, especially for users expecting their initial, enthusiast-level performance. That said, it\u0026rsquo;s not immediately clear which of the first two strategies are most cost-effective or what that should even mean in this temporal context!\nAs such, we\u0026rsquo;ll consider cost-effectiveness as the average performance of each system per dollar and plot the results of all possible choices below.7\nContour plot of cost-efficiency of PC building strategies where you are upgrading the main system components (and reselling the used ones) on a fixed frequency. Interestingly, the optimal price point has actually fallen slightly compared to considering a new PC build in isolation! There is an interesting trade-off here.\nThe longer you wait between upgrades, the more time your build\u0026rsquo;s performance will degrade, but The longer you wait between upgrades, the more performance you can buy for the same fixed dollar amount Again, note how much leeway you have while still having extremely high cost-efficiency! There is a huge range with prices between ~$600 and ~$1000, and an upgrade frequency above 3 years that are within 10% of the best cost efficiency possible.\nHow much do you need to spend to \u0026ldquo;future-proof\u0026rdquo; a new build? # In general, \u0026ldquo;future proofing\u0026rdquo; a PC is ill-advised. It is significantly more cost-effective to purchase a moderately powerful system and plan on periodic upgrades, as we saw above.\nNevertheless, many new PC builders are tempted by this idea, so let\u0026rsquo;s explore what it would actually take. In contrast to our earlier analysis focusing on average performance, here we are interested in the minimum performance of a system over time.\nIn other words, we\u0026rsquo;re interested in targets such as \u0026ldquo;I always want at least a Great build, compared to current systems, for the least cost possible.\u0026rdquo; Optimizing this metric yields the following build strategies.\nPerformance Target Optimal Strategy Minimum (~35%) $650 build upgraded every 3.7 years Entry (~45%) $709 build upgraded every 3.0 years Fair (~72%) $927 build upgraded every 1.7 years Good (~85%) $1259 build upgraded every 1.8 years Very Good (~96%) $1648 build upgraded every 2.1 years Great (~105%) $1942 build upgraded every 2.2 years Excellent (~132%) $2764 build8 upgraded every 2.4 years Exceptional (~170%) $3861 build upgraded every 2.5 years You can immediately see from the relatively short upgrade frequencies that this \u0026ldquo;future proofing\u0026rdquo; is a less cost-effective focus. Remember, we optimized for lowest cost, so longer time periods would be even more costly.\nAdditionally, note how the number of years between upgrades drops sharply as we move beyond the point of highest cost efficiency.\nVisualization of three PC upgrade strategies corresponding to targeting always better than a Minimum, Fair, or Great build, respectively. Another way to interpret this data is to compare each strategy to its equivalent with the same average performance. The latter are all more cost-effective, albeit with an average of ~30% larger performance swings, starting out faster and ending up slower between each upgrade.\nTier Name Optimal Minimum Performance Strategy Equivalent Average Performance Strategy Minimum $650 build upgraded every 3.7 years,$440 upgrades and $197 sales,long-term cost of $86 per year $734 build upgraded every 5.7 years,$498 upgrades and $173 sales,long-term cost of $81 per year Entry $709 build upgraded every 3.0 years,$481 upgrades and $228 sales,long-term cost of $108 per year $804 build upgraded every 4.5 years,$545 upgrades and $209 sales,long-term cost of $100 per year Fair $927 build upgraded every 1.7 years,$628 upgrades and $300 sales,long-term cost of $222 per year $1273 build upgraded every 4.0 years,$862 upgrades and $260 sales,long-term cost of $191 per year Good $1259 build upgraded every 1.8 years,$853 upgrades and $346 sales,long-term cost of $320 per year $1950 build upgraded every 5.3 years,$1321 upgrades and $266 sales,long-term cost of $261 per year Very Good $1648 build upgraded every 2.1 years,$1117 upgrades and $400 sales,long-term cost of $392 per year $2576 build upgraded every 6.1 years,$1745 upgrades and $276 sales,long-term cost of $325 per year Great $1942 build upgraded every 2.2 years,$1316 upgrades and $460 sales,long-term cost of $445 per year $3051 build upgraded every 6.5 years,$2067 upgrades and $287 sales,long-term cost of $374 per year As with all models, you should take this analysis as a rough guideline and factor in your own special considerations in your plans. Do some of your own research!\nYou\u0026rsquo;re bound to find a special deal or unusually cheap offer, which will necessarily deviate from the predictions shown here. Any big upcoming hardware announcements could also warrant briefly delaying an upgrade.\nSee also and references # For additional details and access to the dataset used in this analysis, refer to \u0026ldquo;Historical PC Build Data\u0026rdquo;. PCPartPicker, an invaluable resource for planning out PC builds, offering customizable component selections, simple compatibility checks, price discovery, and many useful product filters. They also provide their own series of recommended build guides as well as a massive repository of community builds. The resources, advice, and support available at /r/buildapc, which is an online community dedicated to custom PC assembly. The inline linked Wikipedia articles on the 2020-2023 global chip shortage, product binning, SLI, logistic functions, and heteroskedasticity. Our data scraping sources, Logical Increments and PassMark, with help from the Internet Archive. I accept that Logical Increments is not really a gold standard in terms of optimal PC builds, but it is quite good as a rough guideline, especially since we are moreso concerned with relative improvements in performance over time.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSimilarly, PassMark\u0026rsquo;s methodologies are definitely not perfect and have some biases, but they should be fine for comparing relative changes over time.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe increase is \u0026ldquo;steady\u0026rdquo; in an exponential sense, note the logarithmic scale in the plot.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhether SLI actually improves performance depends heavily on the use case and is beyond the scope of this discussion. Generally, it can either scale up perfectly, degrade performance compared to a single GPU, or fall somewhere in between.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIndeed, the automatically computed baseline here is somewhere around pairing an i7 7700K with an RX 470, which is not far off from the Ryzen 5 2600 + RX 570 builds of 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nReally, \u0026ldquo;average\u0026rdquo; here means the geometric mean since the tiers\u0026rsquo; prices roughly increase in constant ratios, not constant dollar amounts.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is much more subtle than you might expect. Computer performance improvement, and degradation relative to future systems, is exponential. Then, the normalization of cost must include the initial system price, the costs of future upgrades, and the funds recouped from selling older components. Moreover, these should all be adjusted for inflation, but that is already baked into our analysis at this point.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAgain, don\u0026rsquo;t put too much emphasis on any of the builds that are this expensive. Builds beyond ~$2000 do not have very stable properties across time.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"16 May 2024","externalUrl":null,"permalink":"/posts/optimal-pc-build-model/","section":"Posts","summary":"What is the optimal amount to spend on a computer? How often should its components be upgraded? We review historical data and build a plausible model of computer performance over time.","title":"Modeling Optimal PC Building Decisions","type":"posts"},{"content":"","date":"12 February 2024","externalUrl":null,"permalink":"/tags/remote-work/","section":"Tags","summary":"","title":"Remote-Work","type":"tags"},{"content":"","date":"12 February 2024","externalUrl":null,"permalink":"/tags/return-to-office/","section":"Tags","summary":"","title":"Return-to-Office","type":"tags"},{"content":"This is an extra post to accompany \u0026ldquo;On the Time Burden of Office Work\u0026rdquo;, which focused on the effective changes in pay and leisure time between remote positions and in-person ones.\nHere, we use the same calculations to populate tables for other weekly hybrid schedules. The horizontal rows indicate the number of in-office days per week in your current role, while the vertical columns represent the same quantity in your next role.\nNaturally, a positive percentage signifies an increase in pay or leisure time resulting from the job change and a negative percentage denotes a decrease.\nChange in effective wages # For example, moving from a fully remote schedule to 3 days in-office implies a 14% pay cut.\nCurrent \\ Next Fully remote 1 day 2 days 3 days 4 days Fully in-person Fully remote +0% -5% -10% -14% -18% -21% 1 day +5% +0% -5% -9% -13% -17% 2 days +11% +5% +0% -5% -9% -13% 3 days +16% +10% +5% +0% -4% -8% 4 days +21% +15% +10% +5% +0% -4% Fully in-person +27% +20% +15% +9% +4% +0% Change in weekday leisure time # For example, moving from a fully remote schedule to 3 days in-office implies a 28% cut in weekday leisure time.\nCurrent \\ Next Fully remote 1 day 2 days 3 days 4 days Fully in-person Fully remote +0% -9% -19% -28% -37% -46% 1 day +10% +0% -10% -20% -31% -41% 2 days +23% +11% +0% -11% -23% -34% 3 days +39% +26% +13% +0% -13% -26% 4 days +59% +44% +29% +15% +0% -15% Fully in-person +86% +69% +52% +35% +17% +0% Change in overall leisure time # For example, moving from a fully remote schedule to 3 days in-office implies a 19% cut in overall leisure time.\nCurrent \\ Next Fully remote 1 day 2 days 3 days 4 days Fully in-person Fully remote +0% -6% -13% -19% -25% -32% 1 day +7% +0% -7% -13% -20% -27% 2 days +14% +7% +0% -7% -14% -22% 3 days +23% +16% +8% +0% -8% -16% 4 days +34% +25% +17% +8% +0% -8% Fully in-person +46% +37% +28% +18% +9% +0% ","date":"12 February 2024","externalUrl":null,"permalink":"/extra/hybrid-work-pay-leisure-tables/","section":"Extra","summary":"A reference for pay and work-life balance shifts implied by changes in remote working schedules.","title":"Tables of Hybrid Work's Impact on Effective Pay and Free Time","type":"extra"},{"content":"","date":"12 February 2024","externalUrl":null,"permalink":"/tags/time-management/","section":"Tags","summary":"","title":"Time-Management","type":"tags"},{"content":"Let\u0026rsquo;s get one thing out of the way first, there are obviously some benefits to in-person work even when remote work is possible. It can foster better team collaboration, lessen communication errors, improve networking and job advancement for junior team members, etc.\nThat said, let\u0026rsquo;s set these factors aside for now and focus solely on the added time commitments faced by employees.\nHow much time does work really take out of a day? # Often, people naturally think of the standard workday as neatly packed into an eight-hour block of time. After all, that is precisely how long you are \u0026ldquo;supposed\u0026rdquo; to spend on the work itself.\nHowever, lets consider the actual overhead surrounding the typical full-time American office job. For the most part, the numbers below are derived from the U.S. Bureau of Labor Statistics\u0026rsquo; 2022 American Time Use Survey.\nTask Time (in-person work) Time (remote work) Morning routine/prep1 30 - 60 minutes 15 - 30 minutes Commute to work2 30 - 60 minutes - Work before lunch 4 hours 4 hours Lunch (non-leisure)3 1 hour 30 minutes Lunch (leisure) - 30 minutes Work after lunch 4 hours 4 hours Commute home 30 - 60 minutes - Total work obligations ~11.25 hours ~8.875 hours Chores / evening routine 2 hours 2 hours Total free time ~2.75 hours ~5.125 hours Indeed, these findings closely mirror the BLS\u0026rsquo;s estimate that the average full-time worker has around 3 hours of free time on weekdays.\nNotably, \u0026ldquo;total work obligations\u0026rdquo; here refers to everything mandatory for work, including time preparing for the workday, commuting to and from the office, and during breaks where employees are restricted to areas in or relatively near the workplace. In other words, it is everything other than time spent on personal activities, chores, or leisure.\nWhat\u0026rsquo;s the impact on effective wages and work-life balance? # From this perspective, compared to fully remote work,\nFully in-office work is effectively A ~21% pay cut, A ~46% loss in weekday leisure time, and a ~32% loss in overall leisure time4 So-called \u0026ldquo;hybrid\u0026rdquo; work with two remote days per week is effectively A ~14% pay cut, A ~28% loss in weekday leisure time, and a ~19% loss in overall leisure time This should align with most people\u0026rsquo;s intuition, particularly given that a significant portion of Americans would be willing to accept a pay cut of up to 20% for the chance to work remotely. For example, see these articles from CBS News, Business Insider, and Fortune.\nI would also wager this is a predominant factor in many office workers arriving late, leaving early, or otherwise finding ways to avoid the office. Unless there is a clear, overwhelming benefit to them or their work, there is very little incentive to spend nearly 2.5 hours of extra overhead in their day just to get the same amount of work done.\nIn fact, one could easily paint a worse picture from the employee\u0026rsquo;s perspective. For instance, in-person work adds significant expenses for transportation, work attire, and meals. An office atmosphere also presents numerous distractions and fewer opportunities for focused, uninterrupted work.\nUnless corporations can address those concerns head-on and honestly communicate with their workforce, they are likely to face an incredible uphill battle indeed. And if they were to blindly push things forward, their most effective (and thus in-demand) workers would be the first to leave for other, more progressive opportunities.\nSee also and references # Feel free to explore the various articles and resources linked throughout the post itself. The extra details in \u0026ldquo;Tables of Hybrid Work\u0026rsquo;s Impact on Effective Pay and Free Time\u0026rdquo;. Talk to any office worker! The range of opinions on this sort of topic varies wildly, but practically anyone should be able to tell you interesting stories. Employees doing the bare minimum to check a box in some report, circumventing tracking measures, corporate leaders sending out wildly inconsistent or tone-deaf communications to huge mailing lists, massive expectation differences from generational divides, etc. This includes everything between waking up and leaving for work (eating, showering, shaving, etc.). There is not an exact equivalent for this in the American Time Use Survey, but this range is comparable the survey\u0026rsquo;s averages for various personal care and eating activities, accounting for three meals in the day and most personal care being split between morning and evening routines. The 50% reduction in time for remote workers across several categories is more arbitrary, but appears consistent with general online consensus.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is slightly longer than the average American commute time to approximate a door-to-door estimate rather than travel time alone. Again, this seems to be consistent with online consensus regarding office workers typically having longer commutes than the average American and the time required to walk to/from parking near the office.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHere, I\u0026rsquo;m considering a lunch break for remote workers to potentially allow for chores and/or general leisure. On the other hand, office workers do not have this privilege and must generally remain in or close to their place of work to accommodate normal business hours and operations.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis overall loss in leisure time is slightly less than just taking 5/7th of the weekday loss because the average full-time worker has ~6 hours of daily leisure time on weekends.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"11 February 2024","externalUrl":null,"permalink":"/posts/time-burden-of-office-work/","section":"Posts","summary":"How much of a pay cut is in-office work compared to remote? All else being equal, maybe a bit more than you\u0026rsquo;d expect.","title":"On the Time Burden of Office Work","type":"posts"},{"content":"","date":"30 January 2024","externalUrl":null,"permalink":"/tags/derivation/","section":"Tags","summary":"","title":"Derivation","type":"tags"},{"content":" Many retirement account providers and individual brokerages provide some sort of calculation for how much you will need to retire and whether you are on track for your goals. However, they are often somewhat opaque and not entirely clear on how they reach the final conclusion that is presented to the user.\nTo this end, we can derive our own formulas to achieve a simplified version of the various calculators that are generally available, regardless of whether you are interested in a simple check on an existing estimate or learning more about how these calculations can be performed in general.\nBelow, we\u0026rsquo;ll derive the formulas used in \u0026ldquo;Analytic Early Retirement Calculator\u0026rdquo;.\nSetting up the problem # In our simple calculations, there are effectively five parameters.\nAnnual savings, \\( s \\) Annual expenses, \\( x \\) Current portfolio value, \\( p \\) Annual return rate (after inflation and taxes), \\( r \\), defaulting to 1.051 Annual withdrawal rate in retirement, \\( w \\), defaulting to 0.042 In some calculations, annual post-tax income (i.e., \\( s + x \\)) is used in the place of savings or expenses.\nWith these parameters defined, we can compute how retirement savings grow over time. Assuming annual savings are continuously invested, we have the following total at time \\( T \\).\n$$ \\begin{aligned} \\text{Savings}(T) \u0026amp;= p \\cdot r^T + \\int_0^T s \\cdot r^t \\text{ d}t \\\\ \u0026amp;= p \\cdot r^T + s \\cdot \\frac{r^T - 1}{\\ln(r)} \\end{aligned} $$\nThe first term represents the growing value of the current portfolio, while the second term represents the value of the continuously invested savings. Both share the same growth rate, \\( r \\).\nThen, you should be able to retire once your withdrawal rate alone provides enough income to cover your annual expenses completely.\n$$ \\begin{aligned} \\text{Savings}(T) \\cdot w \u0026amp;\\geq x \\\\ \\text{Savings}(T) \u0026amp;\\geq \\frac{x}{w} \\end{aligned} $$\nWith these equations, we can solve for any of the parameters in terms of the others.\nCalculating time until retirement # Setting retirement savings equal to the final required portfolio value, we can find the time until retirement.\n$$ \\begin{aligned} \\text{Savings}(T) \u0026amp;= \\frac{x}{w} \\\\ p \\cdot r^T + s \\cdot \\frac{r^T - 1}{\\ln(r)} \u0026amp;= \\frac{x}{w} \\\\ \\left(p + \\frac{s}{\\ln(r)}\\right) \\cdot r^T - \\frac{s}{\\ln(r)} \u0026amp;= \\frac{x}{w} \\\\ r^T \u0026amp;= \\frac{\\frac{x}{w} + \\frac{s}{\\ln(r)}}{p + \\frac{s}{\\ln(r)}} \\\\ r^T \u0026amp;= \\frac{x\\ln(r) + s \\cdot w}{p \\cdot w\\ln(r) + s \\cdot w} \\\\ T \u0026amp;= \\frac{\\ln\\left[x\\ln(r) + s \\cdot w\\right] - \\ln\\left[p \\cdot w\\ln(r) + s \\cdot w\\right]}{\\ln(r)} \\end{aligned} $$\nFor example, someone that makes $50k, saves $7k, and has $25k currently invested should be able to retire in ~40.6 years.\n$$ \\begin{aligned} T \u0026amp;= \\frac{\\ln\\left[x\\ln(r) + s \\cdot w\\right] - \\ln\\left[p \\cdot w\\ln(r) + s \\cdot w\\right]}{\\ln(r)} \\\\ \u0026amp;= \\frac{\\ln\\left[43000 \\cdot \\ln(1.05) + 7000 \\cdot 0.04\\right] - \\ln\\left[25000 \\cdot 0.04 \\cdot \\ln(1.05) + 7000 \\cdot 0.04\\right]}{\\ln(1.05)} \\\\ \u0026amp;\\approx \\frac{7.7740 - 5.7954}{0.04879} \\approx 40.6 \\text{ years} \\end{aligned} $$\nCalculating current savings required to meet retirement goal # In this case, let \\( \\text{income} = s+x \\) represent the user\u0026rsquo;s income. Then, we can solve for the savings needed to retire after a specified number of years.\n$$ \\begin{aligned} \\text{Savings}(T) \u0026amp;= \\frac{\\text{income} - s}{w} \\\\ p \\cdot r^T + s \\cdot \\frac{r^T - 1}{\\ln(r)} \u0026amp;= \\frac{\\text{income} - s}{w} \\\\ p \\cdot w \\cdot r^T + s \\left[ \\frac{w \\left(r^T - 1\\right)}{\\ln(r)} + 1 \\right] \u0026amp;= \\text{income} \\\\ s \u0026amp;= \\ln(r) \\cdot \\frac{\\text{income} - p \\cdot w \\cdot r^T}{w \\left(r^T - 1\\right) + \\ln(r)} \\end{aligned} $$\nFor example, someone that makes $50k with $100k invested must save ~$8,785 annually to retire in 30 years.\n$$ \\begin{aligned} s \u0026amp;= \\ln(r) \\cdot \\frac{\\text{income} - p \\cdot w \\cdot r^T}{w \\left(r^T - 1\\right) + \\ln(r)} \\\\ \u0026amp;= \\ln(1.05) \\cdot \\frac{50000 - 100000 \\cdot 0.04 \\cdot 1.05^{30}}{0.04 \\left(1.05^{30} - 1\\right) + \\ln(1.05)} \\\\ \u0026amp;\\approx 0.04879 \\cdot \\frac{50000 - 17288}{0.13288 + 0.04879} \\approx \\$8785 \\end{aligned} $$\nCalculating current portfolio required to meet retirement goal # Lastly, we can solve for the current investment portfolio you would need to be on track for retiring in a specified number of years.\n$$ \\begin{aligned} \\text{Savings}(T) \u0026amp;= \\frac{x}{w} \\\\ p \\cdot r^T + s \\cdot \\frac{r^T - 1}{\\ln(r)} \u0026amp;= \\frac{x}{w} \\\\ p \u0026amp;= r^{-T} \\cdot \\left[ s \\cdot \\frac{1 - r^T}{\\ln(r)} + \\frac{x}{w} \\right] \\end{aligned} $$\nFor example, someone that makes $50k and saves $10k would need ~$375k of current investments to retire in 15 years.\n$$ \\begin{aligned} p \u0026amp;= r^{-T} \\cdot \\left[ s \\cdot \\frac{1 - r^T}{\\ln(r)} + \\frac{x}{w} \\right] \\\\ \u0026amp;= 1.05^{-15} \\cdot \\left[ 10000 \\cdot \\frac{1 - 1.05^{15}}{\\ln(1.05)} + \\frac{40000}{0.04} \\right] \\\\ \u0026amp;\\approx 0.48102 \\left[ 10000 \\cdot \\frac{-1.0789}{0.04879} + 1000000 \\right] \\approx \\$374650 \\end{aligned} $$\nI would highly encourage playing around with the formulas (especially in the interactive calculator) to see how small tweaks to savings can make a massive difference in your retirement timeline. Otherwise, play around with different assumptions and make the calculations more comprehensive or complicated as you see fit!\nSee also and references # The calculator that uses the formulas discussed in this post at \u0026ldquo;Analytic Early Retirement Calculator\u0026rdquo;. The rationale for the default parameter choices and relevant links in the footnotes of this post. The Bogleheads wiki page on Retirement calculators and spending which details the various spending models used by common retirement calculators. This default growth rate of 5% is a relatively conservative choice for historical stock/bond market returns over long periods of time after adjusting for inflation and taxes. Both NerdWallet\u0026rsquo;s articles and Investopedia\u0026rsquo;s articles suggest something closer to 6%, but it is better to err on the lower side to account for potential downturns over shorter time horizons, higher proportions of bond holdings, etc.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is a rough rule of thumb popularized by the \u0026ldquo;Trinity study\u0026rdquo; that suggested typical retirees can sustain withdrawing 4% of their savings every year without running out of funds. People with unusually long retirement horizons (e.g., those that retire early) often target more conservative values around 3.5% or even 3%. This is a topic of extensive ongoing debate, and you can read more in the Bogleheads wiki articles on Safe withdrawal rates and Trinity study update.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"30 January 2024","externalUrl":null,"permalink":"/posts/deriving-analytic-retirement-estimates/","section":"Posts","summary":"Take a DIY approach to calculating rough retirement timelines. No need for fancy services \u0026ndash; roll your own, either for a secondary sanity check or out of curiosity!","title":"Deriving Analytic Retirement Estimates: A DIY Approach","type":"posts"},{"content":"","date":"30 January 2024","externalUrl":null,"permalink":"/tags/investing/","section":"Tags","summary":"","title":"Investing","type":"tags"},{"content":"","date":"30 January 2024","externalUrl":null,"permalink":"/tags/retirement/","section":"Tags","summary":"","title":"Retirement","type":"tags"},{"content":" When can I retire? Calculating time, savings, and portfolio # Calculation type: Years until retirementRequired annual savingsRequired current portfolio valueCurrent annual income: Current annual savings: Current portfolio value: Years until retirement: Annual return on investment: %Withdrawal rate in retirement: %Show yearly net worth table: The output should be written here, but perhaps you have JavaScript disabled? The key thing to notice here is that your savings rate is far more important for an early retirement than your return on investment. Every cut to your expenses has a dual impact \u0026ndash; it increases the amount of money you are saving for your future and reduces the amount of money you\u0026rsquo;ll need in retirement.\nA depiction of retirement timelines across various savings rates, assuming an initial portfolio value of $0. The bold line represents a 5% annual return and 4% withdrawal rate, while the shaded area reflects a spectrum of conservative to aggressive scenarios. An animated version for various initial portfolios is available on YouTube. Assumptions # All values above are presented in today\u0026rsquo;s dollars (i.e., they reflect current purchasing power) and the calculations assume\nYour \u0026ldquo;current income\u0026rdquo; is post-tax. Your cost of living (income minus savings) remains steady, adjusted for inflation. Your \u0026ldquo;annual savings\u0026rdquo; are continuously invested. The \u0026ldquo;annual return on investment\u0026rdquo; is after taxes and inflation. Your goal is to retire indefinitely (i.e., your net worth in retirement will never shrink). See also and references # The underlying calculation details in \u0026ldquo;Deriving Analytic Retirement Estimates: A DIY Approach\u0026rdquo;. My earlier post about \u0026ldquo;The Importance of Saving Early for Retirement\u0026rdquo; and the linked resources therein on budgeting, investing, and planning for retirement. Networthify\u0026rsquo;s Early Retirement Calculator, which heavily inspired this page as I wanted to improve on its assumptions and address some slight inaccuracies. FIRECalc, a more sophisticated retirement calculator based on historical market simulation and backtesting. ","date":"21 December 2023","externalUrl":null,"permalink":"/posts/analytic-early-retirement-calculator/","section":"Posts","summary":"A retirement calculator that estimates your retirement timeline, required annual savings, and assesses your progress toward retirement goals.","title":"Analytic Early Retirement Calculator","type":"posts"},{"content":"","date":"21 December 2023","externalUrl":null,"permalink":"/tags/early-retirement/","section":"Tags","summary":"","title":"Early-Retirement","type":"tags"},{"content":"After playing around with QR code corruptions in \u0026ldquo;Tips on Creating the Smallest Possible QR Codes\u0026rdquo;, I became curious about generating a few animated variants that are valid on every frame.\nThe perfect algorithm candidates for this are simple cellular automata, which are models of computation on a grid that repeatedly update the state of each cell based on the arrangement of its neighbors. For instance, a cell might \u0026ldquo;live\u0026rdquo; or \u0026ldquo;die\u0026rdquo; depending on the state of its nearby cells.\nOn top of this, I also allowed a few caveats for more interesting patterns and ease of generation.\nI ran the automata on a grid nine times finer than the modules of a standard QR code I threw in short periods of noisier updates where the automata\u0026rsquo;s rules were loosened, yielding behavior akin to a Monte Carlo simulated anneal I evaluated the automata\u0026rsquo;s update rules in a random order rather than simultaneously In later passes, I allowed updates to be completely rejected if they created an invalid QR code, effectively altering the random sampling order of cell updates Here, we\u0026rsquo;ll start with a very simple method and then progressively add in enhancements and discuss the issues I ran into along the way.\nThe simple, fixed centers method # The Python package \u0026ldquo;segno\u0026rdquo; offers a somewhat standard method to generate QR codes that incorporate colorful images or GIFs. Their solution is to fix all the position, tracking, and timing modules as well as the center portion of every other module. Such a restriction is shown below.\nA QR code and a visualization of the regions unaffected by this simple strategy (colored in red). The idea here is that most QR code readers sample from the estimated center of each module, so that portion is really all that is needed for readability as long the position and orientation of the code can be accurately determined.\nThe resulting codes are typically valid, although there are cases with complex patterns that might confuse smartphone readers. Despite this, they can generally be scanned with some effort, including the ones below which are maximally filled with white and black.\nThe resulting QR codes after coloring in all allowed pixels with white and black, respectively. Running a few simple automata under these constraints indeed results in valid QR codes. The patterns are also pretty interesting to look at, especially since the fixed regions influence neighboring pixels through the same update rules as the rest of the grid.\nTo me, two particular columns stand out: the right-most one, which essentially dithers the QR code, and the second one, which creates smooth, gyrating blobs.\nAn aside about halting update rules # The examples presented here do not reach a final halting state and continue indefinitely. However, you can create compelling patterns using update rules that do lead to a final, static state.\nFor example, I accidentally stumbled upon one that, in the limit, was equivalent to copying states diagonally down and to the right whenever possible. Despite being mostly uninteresting from an animation perspective, I am quite happy with how these turned out and their relative robustness when scanning.\nGenerated QR codes from a halting update rule that creates diagonal stripes across the data modules. Removing pixel restrictions by explicitly testing for QR code validity # While segno\u0026rsquo;s strategy is effective, it is also pretty heavy-handed. If you\u0026rsquo;ve ever seen artistic QR codes in shops or online, it\u0026rsquo;s apparent that you don\u0026rsquo;t actually need to keep the tracking modules fixed. Moreover, the built-in error correction should allow us to completely alter some of the modules without compromising readability.\nAs such, we could simply try testing the validity of the QR code on each modification. If it ever becomes invalid, we\u0026rsquo;ll just reject the update and try again.\nHowever, if you try this you\u0026rsquo;ll quickly run into the issue that automated QR code readers are way too lenient!1 Below, I\u0026rsquo;ve included three QR codes that are barely recognizable, but still technically valid according to some of the most popular Python decoders.\nThree absurdly corrupted QR codes that both pyzbar and OpenCV can detect and decode \u0026ldquo;correctly\u0026rdquo;. Ultimately, randomized updates will exploit every little idiosyncrasy of these decoding algorithms, yielding QR codes that are entirely unreadable in real-world situations.\nOn top of this, smartphone manufacturers like Apple, Google, and Samsung use proprietary and opaque decoding implementations in their devices. It seems extremely difficult in general to determine what they will be able to read and which patterns will confuse them. Their variety of lenses, auto-focus, auto-exposure, and other post-processing techniques only further complicate the matter.\nImproving QR code readability with robustness filters # To overcome these challenges, we need to enhance the QR codes to be more robust than strictly necessary.\nOne effective technique I settled on was ensuring that the QR code remains valid after running a set of image filters. Rank filters that dilate and erode the image proved to be particularly effective for this purpose, a few of which are visualized below.\nA QR code (top-left corner) and the result of running five different 3x3 and 5x5 rank filters on it. This intuitively makes sense as it prevents the readers from placing too much emphasis on small regions of the code. Indeed, since we\u0026rsquo;re reading directly from a lossless, undistorted image, the readers are free to just look at the center-most pixel in each module, which requires far more accuracy than we are likely to get in practice.\nI also took the liberty of fixing the inner portions of the format information in the upper left and the tracking modules to prevent them from being completely obliterated by the automata\u0026rsquo;s random updates. This adjustment significantly improved the overall detection and readability on smartphones.\nThis looser restriction is visualized below, and you should note how much less of the total QR code is off-limits to the automata compared to the initial method.\nA QR code and a visualization of the regions unaffected by this newer strategy (colored in red). Importantly, this technique allows for entire modules to be corrupted, provided that the error correction can restore the original data. Moreover, the tracking modules can be corrupted slightly, which provides a more artistic bent to overall QR code.\nRunning the automata now gives us the examples below, and I was able to successfully scan them all with an iPhone, Google Lens, Samsung\u0026rsquo;s camera, and others.\nHowever, it\u0026rsquo;s important to note that this method effectively creates a few fixed areas of 5x5 pixels to overcome the filters. This is larger than the fixed regions of the simpler method. Unfortunately, a set of 3x3 filters did not create codes that were robust enough for my liking.\nI also explored blurring filters, rotations, and various resizing algorithms, but they didn\u0026rsquo;t prove as effective as rank filters. Similarly, obscuring regions (i.e., repeatedly cutting out various parts of the image to increase redundancy and prevent dependence on specific modules) did not increase robustness as much as I would have hoped.\nUltimately, I think this could be greatly improved upon, particularly with more insight into common QR code reader implementations and more realistic distortions. That said, I am relatively happy with the result as it stands now, especially since I have already spent thousands of compute hours thus far in my experiments.\nSee also and references # The earlier post about \u0026ldquo;Tips on Creating the Smallest Possible QR Codes\u0026rdquo; and the relevant links therein. The Wikipedia articles on cellular automata and simulated annealing. This is clearly intentional in order to improve their decoding robustness, but it is detrimental for our purposes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"17 October 2023","externalUrl":null,"permalink":"/posts/qr-code-automata/","section":"Posts","summary":"An experiment in using randomized cellular automata to generate QR codes that remain valid on every frame.","title":"Animated Cellular Automaton QR Codes","type":"posts"},{"content":"","date":"17 October 2023","externalUrl":null,"permalink":"/tags/cellular-automata/","section":"Tags","summary":"","title":"Cellular-Automata","type":"tags"},{"content":"","date":"17 October 2023","externalUrl":null,"permalink":"/tags/qr-codes/","section":"Tags","summary":"","title":"Qr-Codes","type":"tags"},{"content":"","date":"17 October 2023","externalUrl":null,"permalink":"/tags/video/","section":"Tags","summary":"","title":"Video","type":"tags"},{"content":" Many Americans underestimate the importance of early retirement savings, often assuming they can delay saving until later in their careers.\nOn the contrary, a 22-year-old college graduate that saves $300/month will have more in retirement than a worker who starts saving $1,100/month at 40 years old.\nAs such, if you are financially stable, it is best and easiest to start saving early!1\nIn this article, we\u0026rsquo;ll explore the significance of saving for retirement in your early working years by focusing on the power of compound growth alone. However, you should keep in mind such savings generally decrease your income taxes, are partially matched by your employer, improve your budgeting capabilities, and provide several other benefits. I think we can all agree that financial security and peace of mind in retirement go a long way to relieving stress in our daily lives.\nComparing different retirement saving strategies # Consider three people who start investing at different times but all retire at 70 years old.\nA high school graduate who consistently saves $375/month from age 18 and continues through their adulthood. This is 15% of a $15/hour paycheck. A coworker who starts saving $700/month at age 30. Their manager who waits and tries to catch up by saving $1,450/month at age 40. Perhaps surprisingly, the early-saving high school student comes out on top. They end up with ~$2.2 million in today\u0026rsquo;s dollars after investing $234k in total.\nOn the other hand, the coworker and manager both end up with ~$1.7 million after investing $336k and $522k, respectively.\nClearly, #1 is the best option here. The high-school student invested the least amount, has the most money in retirement, and didn\u0026rsquo;t have to experience huge, sudden losses of disposable income in the middle of their working career. Starting early with small contributions can yield greater savings than starting later with much larger ones!\nNow, let\u0026rsquo;s visualize some of these fundamental benefits of saving early.\nSaving early is easier # As the saying goes, \u0026ldquo;the best time to start investing was 20 years ago and the second-best time is now.\u0026rdquo;\nIf you plot out the amount you need to save each month for a fixed nest egg at retirement, you find that it becomes exponentially harder the more you wait.\nMonthly savings required to reach $1 million at age 67, based on the age you start investing. Saving longer makes you exponentially more money # Investments grow significantly over time, thanks to the power of exponential compound growth. However, the degree to which this effect snowballs may be unintuitive if you have not worked with it before.\nRemember, the longer your money stays invested, the more it can grow.\nAverage earnings of each invested dollar as the investing time horizon grows longer. Your first years have the biggest impact # Compound growth makes the first years of contributions the most influential. They will always have the longest time to multiply, relative to the rest of the portfolio.\nPercentage of total savings attributed to initial investment years over time. The underlying calculations # To better understand how your choices matter, let\u0026rsquo;s break down the math behind these savings scenarios. You can freely skip this section if you are not interested.\nWe assume that these individuals continuously save a set amount each year, and their investments grow at an annualized rate of 7%. This is around the historical average return of the U.S. stock market over the last century after accounting for inflation.\nThen, with annual savings of \\( s \\) dollars, a time horizon of \\( T \\) years, and an annual growth rate of \\( r \\), you arrive at a final portfolio value of $$ s \\cdot \\int_0^T r^t \\text{ d}t = s \\cdot \\frac{r^T - 1}{\\ln(r)} $$\nIf you\u0026rsquo;re curious about the continuous investment assumption, it is basically equivalent to the more lengthy, discrete calculation of monthly contributions. For example, $$ (\\text{\\$6000 saved annually}) \\cdot \\frac{1.07^{\\text{(45 years)}} - 1}{\\ln(1.07)} \\approx \\text{\\$1,773,827} $$ is quite close to the sum of monthly contributions over the same period, $$ \\sum_{t=0}^{\\text{(45 years)} \\cdot 12 - 1} \\left[ (\\text{\\$500 saved monthly}) \\cdot 1.07^{t/12} \\right] \\approx \\text{\\$1,768,831} $$\nSee also and references # Compound growth favors those who start saving early, so I highly recommend starting as soon as you are able. The following resources are a decent place to start:\nThe /r/personalfinance wiki pages on budgeting, building an emergency fund, and saving for retirement or other goals. These provide overall advice and planning options regardless of what your financial picture currently looks like. For more in-depth information, see the Bogleheads wiki in general, including their general getting started pages and advice on creating simple three-fund investment portfolios or other lazy portfolios. These are more focused on individuals who are already investing, but do provide some guidance on achieving a sound financial footing for other readers. Needless to say, I am not a financial advisor and this should not be construed as financial advice. The content provided here is based on general knowledge and research. Please perform your own research and consult with a qualified financial professional as needed before making any significant financial decisions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"13 September 2023","externalUrl":null,"permalink":"/posts/early-savings-importance/","section":"Posts","summary":"A 22-year-old college graduate that saves $300/month will have more in retirement than a worker who starts saving $1,100/month at 40 years old. Let\u0026rsquo;s talk about it.","title":"The Importance of Saving Early for Retirement","type":"posts"},{"content":" Let\u0026rsquo;s face it, a good password never needs to be longer than 20-40 characters.\nAs of 2023, even a 15-character password is beyond the brute-force abilities of the entire human race.\nHere, we\u0026rsquo;ll start with the fundamentals of measuring password strength, and then demonstrate the range of hackable password lengths \u0026ndash; from solo hackers to entire nations, all the way up to a galactic superpower.\nCaveats abound # This discussion mainly applies to randomly generated passwords with a large character set1 since memorable passwords are generally quite insecure. Predictable patterns are simply easy to guess.\nFor example, an English word in a password contributes ~16 bits of entropy (a measure of information or password strength), equivalent to 2-3 ASCII characters.\nIf your password looks anything like PronounceCoauthor1984!, it\u0026rsquo;s about as secure as k^D5] and can be broken pretty quickly.2 And that\u0026rsquo;s a lot better than most passwords I see.\nIndeed, in my experience, a frightening number of users\u0026rsquo; passwords can be broken in seconds on a consumer-grade desktop. Such attacks typically start with massive lists of popular passwords from prior leaks and common words in many languages. Then, they add special characters, tweak capitalization, and make other modifications as needed.\nThe concept of security level # Cryptographic security is like building a lock that only the right key can open. The strength of the lock depends on how many different keys an attacker would have to try to break in.\nRoughly speaking, if a cryptographic scheme has \u0026ldquo;128-bit security\u0026rdquo;, then an attacker would need to perform \\( 2^{128} \u0026gt; 3 \\cdot 10^{38} \\) operations to decrypt it. That\u0026rsquo;s 300 trillion trillion trillion.\nMany HTTPS connections provide precisely this level of security for the communication channel, even for some large U.S. banking websites.3 The data sent over those connections can be very sensitive and those companies apparently believe 128 bits is sufficient to protect it.\nThere are 95 characters in the printable ASCII set, so any password over \\( \\log_{95}(2^{128}) = 128 / \\log_2(95) \u0026lt; 20 \\) characters is completely useless in this context. At that point, the cryptographic methods used to secure the data become the weakest link, and it is simply easier to break the encryption primitive than attack the password itself.\nThe most secure common encryption methods provide 256 bits of security, which is exponentially more secure.4 In other words, breaking a 256-bit scheme is as challenging as breaking 300 trillion trillion trillion 128-bit ones.\nThe limits of a typical online attack # Now, lets look at the limits of what a practical attack could look like.\nAn online service should have rate limiting so you can\u0026rsquo;t try a hundred passwords all at once, but let\u0026rsquo;s say that the implementation is poor or there is an unknown exploit allowing you to check password validity as often as the network allows.\nAssuming one guess per millisecond over an entire year, you can break ~35 bits of security. $$ \\left(1 \\text{ kHz}\\right) \\cdot \\left(1 \\text{ year}\\right) \\approx 2^{34.9} \\approx 3.2 \\cdot 10^{10} $$\nThat is less than the entropy of keV#]i, a 6-character password.\nHowever, the real concern is in an offline attack where someone has gained access to a database leak or exfiltrated data from the servers themselves. In those cases, you can easily test passwords with all the computational power you have on hand and never have to worry about network delays.\nThe limits of practical offline attacks # For simplicity and conservativeness, we\u0026rsquo;re going to assume that a key can be checked in a password hash or encryption scheme in a single floating point operation. This is pretty unrealistic, but perhaps the attacker has procured specialized ASICs specifically to attack you or a service that you use.\nA lone hacker # As a baseline, you can get up to ~7 TFLOPS of FP32 performance per $100 through an Arc A750, RX 7900 XTX, or RTX 4060 Ti.\nLet\u0026rsquo;s say a lone hacker has $10k to throw around and salvages the rest of their rigs for free from tech recycling or theft. They also siphon free electricity directly from the grid or an unsuspecting industrial company. In our idealized scenario, this yields a guess rate of 700 THz.\nOver 1 year, they can break ~74 bits of security. $$ \\left(700 \\text{ THz}\\right) \\cdot \\left(1 \\text{ year}\\right) \\approx 2^{74.2} \\approx 2.2 \\cdot 10^{22} $$\nFor reference, that\u0026rsquo;s under the entropy of iDif2$isXM\u0026amp;:, a 12-character password.\nRealistically, common attacks just run through every credential in a leak and exploit the users with the weakest passwords. If a hacker is willing to spend a year cracking yours in particular, you are probably a high profile individual and have much bigger issues to worry about.\nA state actor # Since 2018, multiple supercomputers and public distributed systems have achieved performance above 1 exaFLOPS (1 million TFLOPS). It is reasonable to suspect this is within the abilities of the major superpowers of the world, especially through botnets and classified supercomputers. In fact, the U.S. was planning such systems as far back as 2007-2008.\nAs of June 2023, the United States has at least 3.5 EFLOPS of (non-distributed) supercomputing power.5\nOver 1 year, this would break ~87 bits of security. $$ \\left(3.5 \\text{ EHz}\\right) \\cdot \\left(1 \\text{ year}\\right) \\approx 2^{86.5} \\approx 1.1 \\cdot 10^{26} $$\nThis could break y1V\u0026quot;5)C{Kqam=, a 13-character password, but nothing more.\nHuge distributed systems can be more powerful, but are unlikely to be controlled by a single entity. That said, people have managed to run requests on AWS that would have ranked as one of the most powerful supercomputers in the world.6\nThe global computing power # It is difficult to estimate the total computing power of the human race, but it is likely below 10 zettaFLOPs (i.e., 1 thousand ExaFLOPS).7 For comparison, the hash rate of the entire Bitcoin network is currently ~500 EHz.\nOver 1 year, this would break ~98 bits of security. $$ \\left(10 \\text{ ZHz}\\right) \\cdot \\left(1 \\text{ year}\\right) \\approx 2^{98.0} \\approx 3.2 \\cdot 10^{29} $$\nThis could almost break +-5{;C:pPf?JcPy, a 15-character password with ~98.5 bits of entropy.8\nThe limits of physics # There is a sci-fi concept called a Matrioshka brain, which is a massive structure that would consume the entire energy output of a star to power a supercomputer of unbelievable strength.\nWith the power output of our Sun, operating at the theoretical limit of energy consumption and the theoretical limit of thermodynamic efficiency, you can achieve \\({\\approx}1.45 \\cdot 10^{49}\\) Hz of processing power.9 Scaling up to the luminosity of the entire Milky Way galaxy yields \\({\\approx} 3 \\cdot 10^{60}\\) Hz of processing power.\nAn illustration of a Matrioshka brain, all devoted to cracking a single password. Over 1 year, the Sun\u0026rsquo;s brain breaks ~188 bits of security, $$ \\left(1.45 \\cdot 10^{49} \\text{ Hz}\\right) \\cdot \\left(1 \\text{ year}\\right) \\approx 2^{188.2} \\approx 4.6 \\cdot 10^{56} $$ and the Milky Way\u0026rsquo;s brain breaks ~226 bits of security. $$ \\left(3 \\cdot 10^{60} \\text{ Hz}\\right) \\cdot \\left(1 \\text{ year}\\right) \\approx 2^{225.8} \\approx 9.5 \\cdot 10^{67} $$ Over the entire age of the universe, the Milky Way\u0026rsquo;s brain breaks ~260 bits of security. $$ \\left(3 \\cdot 10^{60} \\text{ Hz}\\right) \\cdot \\left(13.8 \\text{ billion years}\\right) \\approx 2^{259.5} \\approx 1.3 \\cdot 10^{78} $$\nEssentially,\nAnnually, the sun can break [2\\p2%i@KabWM4hr\u0026amp;*m|!aok%S.o, a 28-character password with ~184 bits of entropy. Annually, the Milky Way can break KHH|j}\u0026lt;$\u0026gt;o8CnCg^Hc8s#I:',JzW3h^%d$, a 34-character password with ~223 bits of entropy. Over the age of the universe, the Milky Way can break C]Y9.1V+dYrA2[3rl?ZF1K9n=re~hUAVh+BJk|P, a 39-character password with ~256 bits of entropy. Notably, the final calculation that spans the age of our universe at the current luminosity of the entire galaxy barely exceeds 256 bits of security. Under these calculations, humanity will never break such a scheme by sheer brute force.\nIf someone tries to give you a password or API key longer than that, you can explain to them that it is absolutely unnecessary unless they\u0026rsquo;re trying to protect their secrets from an all-powerful God.\nSee also and references # The various links strewn across the footnotes of this post. The extra details in \u0026ldquo;Table of Password Lengths for Various Character Sets and Entropies\u0026rdquo;. The general Wikipedia articles on password strength, security level, and key size. I\u0026rsquo;ll be considering the character set to be the 95 printable ASCII characters, though many services place character restrictions which would decrease the number of possibilities accordingly. Recommendations using other character sets are shown in \u0026ldquo;Table of Password Lengths for Various Character Sets and Entropies\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThese words are from a small Diceware word list (7776 possibilities) provided by the EFF here. If you assume that most people select years from the last 5 decades and tend to only use the 12 main special characters !@#$%^\u0026amp;*-_.?, you find that this password has \\(\\log_2(7776^2 \\cdot 50 \\cdot 12) \\approx 35\\) bits of entropy. The capitalization here does not significantly increase the security of the password.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt\u0026rsquo;s important to note that while the security level might be 128 bits, the actual key lengths may be longer! RSA and ECC keys actually need to be 3072 bits and 256-383 bits long, respectively, to achieve this security level. For more information, see page 55 of NIST Special Publication 800-57 Part 1 Rev. 5.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe may see this change in the next few years as NIST\u0026rsquo;s Post-Quantum Cryptography Standards are finalized. Notably, the quantum Grover\u0026rsquo;s algorithm halves the security level of most symmetric-key algorithms. That said, 256-bit classical / 128-bit quantum security is still considered secure since that search space is considerably beyond current computational capabilities.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is according to the statistics from the TOP500 list https://www.top500.org/statistics/list/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example, an experiment in 2021 reached position 40 on the TOP500 list with \u0026ldquo;4,096 EC2 instances [\u0026hellip;] with a total of 172,692 cores\u0026rdquo;, achieving a performance of ~10 PFLOPS.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt\u0026rsquo;s unclear how reputable this estimate is, but an AI Impacts wiki post claims a total of ~4 ZFLOPS as of Q1 2023. This is pretty consistent with my apparent 2019 estimate of ~1-2 ZHz in one of my favorite footnotes I\u0026rsquo;ve ever written (see page 13 of my undergraduate thesis).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRemember that bits of security are an exponential measure. That last 0.5 bits of security increase the time required from ~1 year to ~1.5 years.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis calculation is due to Arthur Isaac in Matrioshka Brains: Star-Powered Computers and closely matches that of the Computer performance by orders of magnitude Wikipedia article.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"20 August 2023","externalUrl":null,"permalink":"/posts/absurd-password-lengths/","section":"Posts","summary":"A discussion of password strength, brute-force attacks, and the physical limits of the universe.","title":"Absurd Password Lengths and the Computational Limits of Humanity","type":"posts"},{"content":"","date":"20 August 2023","externalUrl":null,"permalink":"/tags/cybersecurity/","section":"Tags","summary":"","title":"Cybersecurity","type":"tags"},{"content":"","date":"20 August 2023","externalUrl":null,"permalink":"/tags/passwords/","section":"Tags","summary":"","title":"Passwords","type":"tags"},{"content":"This is an extra post to accompany \u0026ldquo;Absurd Password Lengths and the Computational Limits of Humanity\u0026rdquo;, which focused on passwords that use the full printable ASCII set. The equivalent recommendations for other character sets are as follows.\nRecall that these bit levels are from the limits computed in the main post for an attack over one year.\n35 bits \u0026ndash; A typical online attack 74 bits \u0026ndash; A lone hacker 87 bits \u0026ndash; A state actor 98 bits \u0026ndash; The global computing power 128 bits \u0026ndash; The maximum security provided by many common encryption methods 188 bits \u0026ndash; An ideal Matrioshka brain using the Sun 226 bits \u0026ndash; An ideal Matrioshka brain using the entire Milky Way 256 bits \u0026ndash; The security of the most secure common encryption methods For example, you need 25 random alphanumeric characters to reach 128 bits of entropy.\nCharacter set 35 bits 74 bits 87 bits 98 bits 128 bits 188 bits 226 bits 256 bits Digits 0-9 11 23 27 30 39 57 69 78 Hexadecimal 0-9A-F 8 17 20 22 29 43 51 58 Alpha a-z 8 16 19 21 28 40 49 55 Alphanumeric a-z0-9 7 15 17 19 25 37 44 50 Case-sensitive Alpha a-zA-Z 7 13 16 18 23 33 40 45 Case-sensitive Alphanumeric a-zA-Z0-9 6 13 15 17 22 32 38 43 Case-sensitive Alphanumeric and Basic Symbols a-zA-Z0-9!@#$%^\u0026amp;*-_.? 6 12 15 16 21 31 37 42 Printable ASCII (without space) 6 12 14 15 20 29 35 40 Printable ASCII 6 12 14 15 20 29 35 39 ","date":"20 August 2023","externalUrl":null,"permalink":"/extra/password-entropies-and-character-sets/","section":"Extra","summary":"A reference for password length recommendations for various character sets and entropy levels.","title":"Table of Password Lengths for Various Character Sets and Entropies","type":"extra"},{"content":"This is an extra post to accompany \u0026ldquo;Tips on Creating the Smallest Possible QR Codes\u0026rdquo; and just contains complete tables of standard QR code data capacities in the numeric, alphanumeric, and binary/byte input modes.\nThe binary/byte input mode is the most general and can encode arbitrary data. The alphanumeric input mode can only encode 0–9, (uppercase) A–Z, space, and $%*+-./:. The numeric input mode can only encode the digits 0-9. Here are the total character limits, with more details available in the following sections.\nEncoding Preference Binary/Byte Alphanumeric Numeric Optimizing for size first 2953 4296 7089 At least M (medium), ~15% error correction 2331 3391 5596 At least Q (quartile), ~25% error correction 1663 2420 3993 Always H (high), ~30% error correction 1273 1852 3057 Optimizing for size first # Note that when preferring small sizes, only low error correction is optimal beyond version 5. At that point, it is always better to drop down to a smaller version if you had a higher error correction level.\nVersion-Error Correction Binary/Byte Alphanumeric Numeric 1-H 1 - 7 1 - 10 1 - 17 1-Q 8 - 11 11 - 16 18 - 27 1-M 12 - 14 17 - 20 28 - 34 1-L 15 - 17 21 - 25 35 - 41 2-Q 18 - 20 26 - 29 42 - 48 2-M 21 - 26 30 - 38 49 - 63 2-L 27 - 32 39 - 47 64 - 77 3-M 33 - 42 48 - 61 78 - 101 3-L 43 - 53 62 - 77 102 - 127 4-M 54 - 62 78 - 90 128 - 149 4-L 63 - 78 91 - 114 150 - 187 5-M 79 - 84 115 - 122 188 - 202 5-L 85 - 106 123 - 154 203 - 255 6-L 107 - 134 155 - 195 256 - 322 7-L 135 - 154 196 - 224 323 - 370 8-L 155 - 192 225 - 279 371 - 461 9-L 193 - 230 280 - 335 462 - 552 10-L 231 - 271 336 - 395 553 - 652 11-L 272 - 321 396 - 468 653 - 772 12-L 322 - 367 469 - 535 773 - 883 13-L 368 - 425 536 - 619 884 - 1022 14-L 426 - 458 620 - 667 1023 - 1101 15-L 459 - 520 668 - 758 1102 - 1250 16-L 521 - 586 759 - 854 1251 - 1408 17-L 587 - 644 855 - 938 1409 - 1548 18-L 645 - 718 939 - 1046 1549 - 1725 19-L 719 - 792 1047 - 1153 1726 - 1903 20-L 793 - 858 1154 - 1249 1904 - 2061 21-L 859 - 929 1250 - 1352 2062 - 2232 22-L 930 - 1003 1353 - 1460 2233 - 2409 23-L 1004 - 1091 1461 - 1588 2410 - 2620 24-L 1092 - 1171 1589 - 1704 2621 - 2812 25-L 1172 - 1273 1705 - 1853 2813 - 3057 26-L 1274 - 1367 1854 - 1990 3058 - 3283 27-L 1368 - 1465 1991 - 2132 3284 - 3517 28-L 1466 - 1528 2133 - 2223 3518 - 3669 29-L 1529 - 1628 2224 - 2369 3670 - 3909 30-L 1629 - 1732 2370 - 2520 3910 - 4158 31-L 1733 - 1840 2521 - 2677 4159 - 4417 32-L 1841 - 1952 2678 - 2840 4418 - 4686 33-L 1953 - 2068 2841 - 3009 4687 - 4965 34-L 2069 - 2188 3010 - 3183 4966 - 5253 35-L 2189 - 2303 3184 - 3351 5254 - 5529 36-L 2304 - 2431 3352 - 3537 5530 - 5836 37-L 2432 - 2563 3538 - 3729 5837 - 6153 38-L 2564 - 2699 3730 - 3927 6154 - 6479 39-L 2700 - 2809 3928 - 4087 6480 - 6743 40-L 2810 - 2953 4088 - 4296 6744 - 7089 At least M (medium), ~15% error correction # Note that only medium error correction is optimal beyond version 4.\nVersion-Error Correction Binary/Byte Alphanumeric Numeric 1-H 1 - 7 1 - 10 1 - 17 1-Q 8 - 11 11 - 16 18 - 27 1-M 12 - 14 17 - 20 28 - 34 2-Q 15 - 20 21 - 29 35 - 48 2-M 21 - 26 30 - 38 49 - 63 3-Q 27 - 32 39 - 47 64 - 77 3-M 33 - 42 48 - 61 78 - 101 4-Q 43 - 46 62 - 67 102 - 111 4-M 47 - 62 68 - 90 112 - 149 5-M 63 - 84 91 - 122 150 - 202 6-M 85 - 106 123 - 154 203 - 255 7-M 107 - 122 155 - 178 256 - 293 8-M 123 - 152 179 - 221 294 - 365 9-M 153 - 180 222 - 262 366 - 432 10-M 181 - 213 263 - 311 433 - 513 11-M 214 - 251 312 - 366 514 - 604 12-M 252 - 287 367 - 419 605 - 691 13-M 288 - 331 420 - 483 692 - 796 14-M 332 - 362 484 - 528 797 - 871 15-M 363 - 412 529 - 600 872 - 991 16-M 413 - 450 601 - 656 992 - 1082 17-M 451 - 504 657 - 734 1083 - 1212 18-M 505 - 560 735 - 816 1213 - 1346 19-M 561 - 624 817 - 909 1347 - 1500 20-M 625 - 666 910 - 970 1501 - 1600 21-M 667 - 711 971 - 1035 1601 - 1708 22-M 712 - 779 1036 - 1134 1709 - 1872 23-M 780 - 857 1135 - 1248 1873 - 2059 24-M 858 - 911 1249 - 1326 2060 - 2188 25-M 912 - 997 1327 - 1451 2189 - 2395 26-M 998 - 1059 1452 - 1542 2396 - 2544 27-M 1060 - 1125 1543 - 1637 2545 - 2701 28-M 1126 - 1190 1638 - 1732 2702 - 2857 29-M 1191 - 1264 1733 - 1839 2858 - 3035 30-M 1265 - 1370 1840 - 1994 3036 - 3289 31-M 1371 - 1452 1995 - 2113 3290 - 3486 32-M 1453 - 1538 2114 - 2238 3487 - 3693 33-M 1539 - 1628 2239 - 2369 3694 - 3909 34-M 1629 - 1722 2370 - 2506 3910 - 4134 35-M 1723 - 1809 2507 - 2632 4135 - 4343 36-M 1810 - 1911 2633 - 2780 4344 - 4588 37-M 1912 - 1989 2781 - 2894 4589 - 4775 38-M 1990 - 2099 2895 - 3054 4776 - 5039 39-M 2100 - 2213 3055 - 3220 5040 - 5313 40-M 2214 - 2331 3221 - 3391 5314 - 5596 At least Q (quartile), ~25% error correction # Note that only quartile error correction is optimal beyond version 4.\nVersion-Error Correction Binary/Byte Alphanumeric Numeric 1-H 1 - 7 1 - 10 1 - 17 1-Q 8 - 11 11 - 16 18 - 27 2-H 12 - 14 17 - 20 28 - 34 2-Q 15 - 20 21 - 29 35 - 48 3-H 21 - 24 30 - 35 49 - 58 3-Q 25 - 32 36 - 47 59 - 77 4-H 33 - 34 48 - 50 78 - 82 4-Q 35 - 46 51 - 67 83 - 111 5-Q 47 - 60 68 - 87 112 - 144 6-Q 61 - 74 88 - 108 145 - 178 7-Q 75 - 86 109 - 125 179 - 207 8-Q 87 - 108 126 - 157 208 - 259 9-Q 109 - 130 158 - 189 260 - 312 10-Q 131 - 151 190 - 221 313 - 364 11-Q 152 - 177 222 - 259 365 - 427 12-Q 178 - 203 260 - 296 428 - 489 13-Q 204 - 241 297 - 352 490 - 580 14-Q 242 - 258 353 - 376 581 - 621 15-Q 259 - 292 377 - 426 622 - 703 16-Q 293 - 322 427 - 470 704 - 775 17-Q 323 - 364 471 - 531 776 - 876 18-Q 365 - 394 532 - 574 877 - 948 19-Q 395 - 442 575 - 644 949 - 1063 20-Q 443 - 482 645 - 702 1064 - 1159 21-Q 483 - 509 703 - 742 1160 - 1224 22-Q 510 - 565 743 - 823 1225 - 1358 23-Q 566 - 611 824 - 890 1359 - 1468 24-Q 612 - 661 891 - 963 1469 - 1588 25-Q 662 - 715 964 - 1041 1589 - 1718 26-Q 716 - 751 1042 - 1094 1719 - 1804 27-Q 752 - 805 1095 - 1172 1805 - 1933 28-Q 806 - 868 1173 - 1263 1934 - 2085 29-Q 869 - 908 1264 - 1322 2086 - 2181 30-Q 909 - 982 1323 - 1429 2182 - 2358 31-Q 983 - 1030 1430 - 1499 2359 - 2473 32-Q 1031 - 1112 1500 - 1618 2474 - 2670 33-Q 1113 - 1168 1619 - 1700 2671 - 2805 34-Q 1169 - 1228 1701 - 1787 2806 - 2949 35-Q 1229 - 1283 1788 - 1867 2950 - 3081 36-Q 1284 - 1351 1868 - 1966 3082 - 3244 37-Q 1352 - 1423 1967 - 2071 3245 - 3417 38-Q 1424 - 1499 2072 - 2181 3418 - 3599 39-Q 1500 - 1579 2182 - 2298 3600 - 3791 40-Q 1580 - 1663 2299 - 2420 3792 - 3993 Always H (high), ~30% error correction # Version-Error Correction Binary/Byte Alphanumeric Numeric 1-H 1 - 7 1 - 10 1 - 17 2-H 8 - 14 11 - 20 18 - 34 3-H 15 - 24 21 - 35 35 - 58 4-H 25 - 34 36 - 50 59 - 82 5-H 35 - 44 51 - 64 83 - 106 6-H 45 - 58 65 - 84 107 - 139 7-H 59 - 64 85 - 93 140 - 154 8-H 65 - 84 94 - 122 155 - 202 9-H 85 - 98 123 - 143 203 - 235 10-H 99 - 119 144 - 174 236 - 288 11-H 120 - 137 175 - 200 289 - 331 12-H 138 - 155 201 - 227 332 - 374 13-H 156 - 177 228 - 259 375 - 427 14-H 178 - 194 260 - 283 428 - 468 15-H 195 - 220 284 - 321 469 - 530 16-H 221 - 250 322 - 365 531 - 602 17-H 251 - 280 366 - 408 603 - 674 18-H 281 - 310 409 - 452 675 - 746 19-H 311 - 338 453 - 493 747 - 813 20-H 339 - 382 494 - 557 814 - 919 21-H 383 - 403 558 - 587 920 - 969 22-H 404 - 439 588 - 640 970 - 1056 23-H 440 - 461 641 - 672 1057 - 1108 24-H 462 - 511 673 - 744 1109 - 1228 25-H 512 - 535 745 - 779 1229 - 1286 26-H 536 - 593 780 - 864 1287 - 1425 27-H 594 - 625 865 - 910 1426 - 1501 28-H 626 - 658 911 - 958 1502 - 1581 29-H 659 - 698 959 - 1016 1582 - 1677 30-H 699 - 742 1017 - 1080 1678 - 1782 31-H 743 - 790 1081 - 1150 1783 - 1897 32-H 791 - 842 1151 - 1226 1898 - 2022 33-H 843 - 898 1227 - 1307 2023 - 2157 34-H 899 - 958 1308 - 1394 2158 - 2301 35-H 959 - 983 1395 - 1431 2302 - 2361 36-H 984 - 1051 1432 - 1530 2362 - 2524 37-H 1052 - 1093 1531 - 1591 2525 - 2625 38-H 1094 - 1139 1592 - 1658 2626 - 2735 39-H 1140 - 1219 1659 - 1774 2736 - 2927 40-H 1220 - 1273 1775 - 1852 2928 - 3057 ","date":"1 August 2023","externalUrl":null,"permalink":"/extra/qr-character-limits/","section":"Extra","summary":"A reference for standard QR code data capacities in the common input modes.","title":"Complete Tables of QR Code Character Limits","type":"extra"},{"content":" The basic advice # If you just want to know how to make the smallest possible standard QR code for a website,\nUse uppercase and no more than 25 characters (0–9, A–Z, space, and $%*+-./:). If using non-alphanumeric characters, use no more than 17 characters. Include http:// or https:// at the start of your URL for compatibility with all QR readers. This effectively limits you to 18 alphanumeric characters or 10 non-alphanumeric characters. This will give you a 21x21 pixel (version 1) standard QR code.\nTwo QR codes for the same URL. Using uppercase takes 21x21 pixels (left) while lowercase takes 25x25 pixels (right). You can generate these on your own with these two Python commands.\nimport segno segno.make_qr(\u0026#34;HTTPS://RYANAGIBSON.COM\u0026#34;).show() segno.make_qr(\u0026#34;https://ryanagibson.com\u0026#34;).show() What about error correction? # When creating small QR codes, you may want to enforce specific error correction levels. Higher correction levels enable faster and easier scanning, but decrease the maximum number of characters that you can fit in a 21x21 QR code.\nFour levels are available, each offering different corruption tolerances and data storage capacities.\nError correction level L (Low) M (Medium) Q (Quartile) H (High) Data corruption allowed ~7% ~15% ~25% ~30% Error correction level Alphanumeric character limit Byte character limit L (Low) 25 17 M (Medium) 20 14 Q (Quartile) 16 11 H (High) 10 7 For example, the following QR codes are all for HTTPS://RYGI.ME with the maximum amount of data corruption shown.1 Despite this, they should still scan correctly.\nLeft to right: QR codes with low, medium, quartile, and high error correction. Top: codes with no corruptions. Bottom: the same codes with data sections shown in blue and mock corruptions shown in red. As usual, error correction also lets you add in artistic embellishments, even with the tiny size! This obviously affects the error correction\u0026rsquo;s effectiveness, but can make your QR code more eye-catching. Here are a few examples I played around with.\nEight QR codes that all point to the same website. They have been intentionally corrupted to make the design more interesting and unique while maintaining readability. In fact, with slight tweaks to the URL, you can artistically corrupt the QR code beyond the theoretical limits of error correction! This would be most useful if you set up redirects to the desired page, but short domains are often explicitly set up for redirection anyway.\nBelow are two examples where a 15x5 pattern is incorporated into the design. This region of 75 pixels is relatively massive considering that a version 1 QR code has a data capacity of only 136 bits. Indeed, the corrupted region is more than 150% the maximum amount this QR code can normally correct!\nLeft: QR code for HTTP://RYGI.ME/+. Right: QR code for HTTP://RYGI.ME/8. They have been very heavily corrupted for design purposes (shown in blue), but still scan correctly. Complete tables of QR code character limits # If you\u0026rsquo;re interested in a more comprehensive reference of the character limits for QR codes, please refer to \u0026ldquo;Complete Tables of QR Code Character Limits\u0026rdquo;.\nSee also and references # The segno Python package for generating standard QR codes and the significantly less supported (and seemingly proprietary) Micro QR codes. The smallest Micro QR code is 11x11 pixels, but could only hold enough data to store a URL (with the http:// prefix) in the 15x15 pixel variant. That one could hold up to HTTP://RYGI.ME. The largest Micro QR code is 17x17 pixels, which could hold up to HTTP://RYANGIBSON.DEV. Wikipedia\u0026rsquo;s QR Code article for a general overview of the standard, its design, features, etc. Denso Wave\u0026rsquo;s What is a QR Code? which discusses more advanced concepts and features of the code. One interesting feature is to split the data into multiple chunks, each with its own encoding, for even more optimal storage. The last one with high error correction drops the HTTPS://, so it will scan as text rather than a URL on some systems. For websites, version 1-H is essentially impossible since HTTP://R.G is already 10 characters and single-character TLDs do not currently exist.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"1 August 2023","externalUrl":null,"permalink":"/posts/small-qr-codes/","section":"Posts","summary":"Some brief tips and discussion on how to make tiny QR codes with a focus on web URLs.","title":"Tips on Creating the Smallest Possible QR Codes","type":"posts"},{"content":"","date":"25 July 2023","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"Github","type":"tags"},{"content":"","date":"25 July 2023","externalUrl":null,"permalink":"/tags/steganography/","section":"Tags","summary":"","title":"Steganography","type":"tags"},{"content":" What is steganography? # In short, steganography is the art of concealing information within another, non-secret message, much like the use of invisible ink on a seemingly innocuous letter.1 The idea is that you could pass the message through many untrusted carriers, such as the internet, without arousing suspicion from most observers.\nAlice transfers a hidden message to Bob over the internet within a seemingly unremarkable image. In today\u0026rsquo;s digital age, you may be surprised as to how much data can be crammed into a file without changing it much at all. For example, below are three tiny 220x220 photos of a flower, but\nOne contains the entire, uncompressed text of the United States Constitution. Another contains the entire text of Shakespeare\u0026rsquo;s Macbeth, containing a little under 20k words. These tiny flowers look practically identical, but two of them hide thousands of words of information. Can you tell which is which? We\u0026rsquo;ll delve into how this is possible in How does (digital) steganography work?, but first let\u0026rsquo;s explore some real-world examples.\nReal-world examples of steganography # We\u0026rsquo;ll start with some physical, non-digital instances.\nThe EURion constellation # One of the simplest methods to hide data is to overlay a pattern in the hopes that it can be recovered later.\nFor example, many banknotes worldwide contain a precise arrangement of circles designed to allow printers and imaging software to combat counterfeiting operations. This has never been officially publicized, but is informally called the \u0026ldquo;EURion constellation\u0026rdquo; and has been integrated into at least ~60 countries\u0026rsquo; currencies.\nIf you happen to have a scanner and some cash on hand, you can try copying one of these banknotes. Depending on the model and brand of the scanner, it might refuse to copy or intentionally corrupt the print by adding stripes across the bill! The one that I own tends to forcibly stop the print halfway through.\nLeft: the specific pattern of circles in the EURion constellation. Middle: A portion of the back of an American $20 bill. Right: Same as the middle, but with the various constellations highlighted in green. Several other examples of the EURion constellation on British, German, and Euro banknotes. Some are more creative with their inclusion into the design than others. Printer \u0026ldquo;Machine Identification Codes\u0026rdquo; # In another covert application of steganography, many color printers use tiny yellow dots that are invisible to the naked eye to overlay a tracking watermark. These encode the serial number of the printer and some date and time information across every printed page.\nThis is also rumored to be one of the reasons why some printers refuse to print black-and-white documents when they are running low on color ink.\nThe existence of this technology remained unknown to the public for around two decades as it was developed under secret agreements with various national governments to enhance their forensic tracing capabilities. As a result, it\u0026rsquo;s been used to track down counterfeiters and whistleblowers across the world.\nAn image of text printed from a Laserjet printer. Blue light makes the Machine Identification Code visible, consisting of scattered yellow dots that are ~0.1mm wide. Steganography in video games # Game developers also use steganography to identify the author of screenshots or gameplay videos, especially when they include cheating, abuse, or unauthorized use of private servers.\nIn the 2000s, Blizzard implemented very faint watermarks on screenshots of World of Warcraft which contained repeating patterns of dots across the entire screen. These patterns, developed by Digimarc, encoded various details of the user\u0026rsquo;s account and the server that they were logged into. Like the other examples above, this screenshot tagging remained entirely secret for the first few years of its existence.\nTwo examples of the watermarks used in World of Warcraft, heavily post-processed to reveal the hidden pattern. Similarly, Microsoft encoded hardware information in the user interface of the Xbox 360\u0026rsquo;s early builds. Each console\u0026rsquo;s animations were unique, which allowed the company to crack down on potential leakers. At the time, the employees were under NDAs and would be subject to civil penalties for disclosing nonpublic information about the console\u0026rsquo;s development.\nHow does (digital) steganography work? # In the realm of digital steganography, there are many different techniques, but one of the simplest is \u0026ldquo;Least Significant Bit\u0026rdquo; (LSB) steganography.\nBasically, the method takes advantage of the fact that most data formats encode information in binary numbers, and the least significant bits of these have the smallest impact on the overall value. By replacing these unimportant bits with a secondary message, we can hide data without making any apparent changes to the file\u0026rsquo;s original appearance or meaning.\nFor example, a common image encoding is to store how much red, green, and blue (RGB) is in each pixel with one byte for each color. These values range from 0 to 255 and we can usually change them slightly without most people noticing. Human senses are just far too imprecise to tell the difference, especially when you\u0026rsquo;re not looking for it!\nAn example of how we can replace the LSBs of an image\u0026rsquo;s pixels to encode a hidden message. The resulting change is nigh-impossible to visually detect. However, this hidden data can be easily exposed in a \u0026ldquo;visual attack\u0026rdquo; where we inspect the LSBs of the image. For instance, if we perform this attack on the three flowers shown at the start of this post, the differences become obvious.\nTop: the three flower images from the start of this blog post. Bottom: a visualization of the least significant bits of each image. The original image is on the right, and you can faintly see the flower\u0026rsquo;s outline in its LSBs. In contrast,\nThe one on the left appears completely random2 since it contained the contents of Hamlet. The one in the middle contained the uncompressed text of the U.S. Constitution and you can visually confirm that the data only takes up the first ~3/4 of the image. In general, steganographic techniques and their adversarial \u0026ldquo;steganalysis\u0026rdquo; counterparts are constantly evolving. More advanced algorithms than this one will minimize changes to the original image\u0026rsquo;s statistics and would only be detectable with much more sophisticated methods.\nOn the other hand, this simple technique lets us store a considerable amount of data! This is a direct consequence of the use of binary encoding since the last bit in each byte can only change the color by 1/255 (~0.4%) despite taking up 1/8th (12.5%) of the data itself.\nIndeed, in the flower images above we\u0026rsquo;ve replaced a whole 25% of the actual image data but only altered around 1% of the color information. There is a significant trade-off between the amount of hidden data and the impact on the visual quality of the image.3\nA demonstration of the trade-off between the number of LSBs used to hide data in an image and the corresponding loss of the original color information. More creative steganography techniques # While we\u0026rsquo;ve provided a reasonable introduction to the basic ideas, there is an abundance of more interesting methods, so we\u0026rsquo;ll briefly mention some of them here.\nText steganography: Messages can be hidden within the formatting, whitespace, or invisible characters of a text itself. Some more intriguing techniques use specific sentence structures or grammatical constructs to impart information. Think of the stereotypical scenario in which you suspect something is amiss when a friend texts you in a particularly unusual writing style. Spread Spectrum: These techniques spread hidden data over a wide range of frequencies, but at a lower amplitude, effectively concealing the covert message beneath the natural noise of the transmission medium. Similar variants are also applicable to images and videos. Audio steganography: In addition to the usual binary techniques, fine manipulation of echoes, harmonics, or the underlying frequency bands can be used to store information. Networking steganography: Many protocols can be manipulated to convey information through calculated usage of (perhaps nonstandard) features, slight manipulation of timing delays between packets, or intentional corruptions that would appear to be typical transmission errors. EOF steganography: End of file markers or headers can be manipulated to hide data outside the intended scope of a file. While not strictly steganography in the traditional sense, it has been repeatedly used in malware and hacking operations, so it is worth mentioning.4 If these topics sound interesting to you, I highly recommend searching the internet and exploring any new techniques that come to mind!\nSee also and references # My Python package, stego-lsb, which I used to generate the steganographed images in this post. It also supports sounds files and arbitrary sequences of binary data. A forum post containing details of the steganographic methods used in World of Warcraft. A Hacker News thread discussing the tracking methods used in Xbox 360 NDA beta builds. A Computerphile video on steganographic techniques in images, which includes a discussion of a method for JPEG images that is robust to simple visual attacks. More generally, consider the following Wikipedia articles.\nSteganography EURion constellation Machine Identification Code Coded anti-piracy, which is a pattern of dots used by the film industry since the 1980s to trace the origins of pirated copies. Deniable encryption techniques, where it is generally impossible to prove that any information is encrypted at all. In fact, steganography comes from Greek word \u0026ldquo;steganographia\u0026rdquo;, which literally means something akin to \u0026ldquo;hidden writing\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nObviously, it\u0026rsquo;s not actually random since this is just compressed English text. In practice, the hidden data should probably be encrypted in some way that increases the apparent randomness. Otherwise, steganography simply becomes an exercise in security through obscurity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf the transmission channel is noisy, a certain amount of error correction would also need to be included, which will necessarily decrease the amount of data available for use. This includes electrical interference on the wire, image compression, audio being played through physical loudspeakers rather than in a perfect digital medium, etc.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is sometimes referred to as stegomalware.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"25 July 2023","externalUrl":null,"permalink":"/posts/steganography-intro/","section":"Posts","summary":"A general introduction to hiding information in plain sight, its uses in the real-world, and how it works in digital mediums.","title":"Steganography: Hiding Data Inside Data","type":"posts"},{"content":"I considered several other domain names for this website, which are listed here. Indeed, all the following domains should redirect to this site until at least mid-2025 when they come up for renewal.\nVariations of my name # These are fairly straightforward and simply incorporate my name in obvious ways.\nhttps://ryanagibson.com/ https://ryanalexandergibson.com/ https://ryanalexgibson.com/ https://ryangibson.dev/ More creative variations # These are a bit more creative and fun, but they may have limited use in professional circles.\nhttps://ragibs.com/ https://rgibson.dev/ https://ryagi.com/ https://ryalg.com/ https://ryalgi.com/ https://ryanalyze.com/ https://ryanexplores.com/ https://rygib.com/ This one is a short domain for convenience or perhaps when space is extremely limited.\nhttps://rygi.me/ Based on my GitHub username # https://ragibson.dev/ https://ragibson.net/ https://ragibson.org/ ","date":"8 July 2023","externalUrl":null,"permalink":"/posts/alternate-domain-names/","section":"Posts","summary":"","title":"Alternate Domain Names","type":"posts"},{"content":"","date":"8 July 2023","externalUrl":null,"permalink":"/tags/domains/","section":"Tags","summary":"","title":"Domains","type":"tags"},{"content":"","date":"8 July 2023","externalUrl":null,"permalink":"/tags/about/","section":"Tags","summary":"","title":"About","type":"tags"},{"content":" Hi! I\u0026rsquo;m Ryan # I am currently working as a Quantitative Analyst at Wells Fargo, primarily focusing on risk model research, development, maintenance, and monitoring. In much of my free time, I work on mathematical, statistical, and programming hobby projects.\nFeel free to reach out with any questions, comments, or ideas, and I\u0026rsquo;ll try to respond reasonably quickly!\nSome of my interests # I earned a B.S. in Computer Science and Pure Mathematics with Highest Honors and Highest Distinction from the University of North Carolina at Chapel Hill, followed by an M.S. in Computer Science.\nAt the time, my research interests included\n🕸️ Network Science 🚅 High Performance Computing 🔐 Cybersecurity 💽 Operating Systems 🔋 Microarchitecture More recently, I\u0026rsquo;ve been additionally working on\n🤖 Machine Learning 🧮 Numerical Analysis Some projects on GitHub # Feel free to look through my repositories list on GitHub (many are private), but I\u0026rsquo;ve linked some projects in the images below!\n","date":"8 July 2023","externalUrl":null,"permalink":"/about/","section":"Ryan A. Gibson","summary":"A brief introduction to my hobbies, interests, work, etc.","title":"About me","type":"page"}]